# Lecture 8: Text Data Processing (Part II)

(Last updated: Mar 8, 2023)

## Preparation

Watch the following videos to understand some math concepts that will be used in this lecture:
- [Vectors](https://www.youtube.com/watch?v=fNk_zzaMoSs)
- [Dot Product](https://www.youtube.com/watch?v=C0sPtQ3wX9o)
- [Cosine Similarity](https://www.youtube.com/watch?v=e9U0QAFbfLI)

## Materials

- [Slides for Lecture 7: Text Data Processing](https://github.com/MultiX-Amsterdam/data-science-book-uva/raw/main/assets/slides/lec7-1.pdf)

## Additional Resources

The following paper explains Topic Modeling and the intuitions:
- [Blei, D. M. (2012). Probabilistic topic models. Communications of the ACM, 55(4), 77-84.](https://www.cs.columbia.edu/~blei/papers/Blei2012.pdf)

The following paper explains how to train word embeddings using Word2Vec:
- [Mikolov, T., et al. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.](https://arxiv.org/pdf/1301.3781.pdf)

The following paper explains how to use the attention mechanism for document classification:
- [Yang, Z., Yang, D., Dyer, C., He, X., Smola, A., & Hovy, E. (2016, June). Hierarchical attention networks for document classification. In Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies (pp. 1480-1489).](https://aclanthology.org/N16-1174/)


