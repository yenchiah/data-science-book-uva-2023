{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial (Text Data Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Last updated: Mar 6, 2023)[^credit]\n",
    "\n",
    "[^credit]: Credit: this teaching material is created by [Robert van Straten](https://github.com/robertvanstraten) under the supervision of [Yen-Chia Hsu](https://github.com/yenchiah)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will familiarize you with the data science pipeline of processing text data. We will go through the various steps involved in the Natural Language Processing (NLP) pipeline for topic modelling and topic classification, including tokenization, lemmatization, and obtaining word embeddings. We will also build a neural network using PyTorch for multi-class topic classification using the dataset.\n",
    "The AG's News Topic Classification Dataset contains news articles from four different categories, making it a nice source of text data for NLP tasks. We will guide you through the process of understanding the dataset, implementing various NLP techniques, and building a model for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the fullowing links to jump to the tasks and assignments:\n",
    "*   [Task 3: Preprocess Text Data](#t3)\n",
    "    *   [Tokenization](#t3-1)\n",
    "    *   [Part-of-speech tagging](#t3-2)\n",
    "    *   [Stemming / lemmatization](#t3-3)\n",
    "    *   [Stopword removal](#t3-4)\n",
    "    *   [Assignment for Task 3](#a3)\n",
    "*   [Task 4: Another option: spaCy](#t4)\n",
    "    *   [Assignment for Task 4](#a4)\n",
    "*   [Task 5: Unsupervised Learning - Topic Modelling](#t5)\n",
    "    *   [Evaluation](#t5-1)\n",
    "    *   [Assignment for Task 5](#a5)\n",
    "*   [Task 6: Word Embeddings](#t6)\n",
    "    *   [Assignment for Task 6](#a6)\n",
    "*   [Task 7: Supervised Learning - Topic Classification](#t7)\n",
    "    *   [Assignment for Task 7](#a7)\n",
    "*   [Optional Assignment / Takeaways](#a8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [AG's News Topic Classification Dataset](https://github.com/mhjabreel/CharCnn_Keras/tree/master/data/ag_news_csv) is a collection of over 1 million news articles from more than 2000 news sources. The dataset was created by selecting the 4 largest classes from the original corpus, resulting in 120,000 training samples and 7,600 testing samples. The dataset is provided by the academic community for research purposes in data mining, information retrieval, and other non-commercial activities. We will use it to demonstrate various NLP techniques on real data, and in the end make 2 models with this data. The files train.csv and test.csv contain all the training and testing samples as comma-separated values with 3 columns: class index, title, and description. Download train.csv and test.csv for the following tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put all the packages that are needed for this tutorial below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import adjusted_mutual_info_score, adjusted_rand_score, confusion_matrix\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from xml.sax import saxutils as su"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block below contains answers for the assignments in this tutorial. **Do not check the answers in the next cell before practicing the tasks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def check_answer_df(df_result, df_answer, n=1):\n",
    "    \"\"\"\n",
    "    This function checks if two output dataframes are the same.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_result : pandas.DataFrame\n",
    "        The result from the output of a function.\n",
    "    df_answer: pandas.DataFrame\n",
    "        The expected output of the function.\n",
    "    n : int\n",
    "        The numbering of the test case.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        assert df_answer.equals(df_result)\n",
    "        print(f\"Test case {n} passed.\")\n",
    "    except:\n",
    "        print(f\"Test case {n} failed.\")\n",
    "        print(\"Your output is:\")\n",
    "        display(df_result)\n",
    "        print(\"Expected output is:\")\n",
    "        display(df_answer)\n",
    "        \n",
    "def check_answer_np(arr_result, arr_answer, n=1):\n",
    "    \"\"\"\n",
    "    This function checks if two output dataframes are the same.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_result : pandas.DataFrame\n",
    "        The result from the output of a function.\n",
    "    df_answer: pandas.DataFrame\n",
    "        The expected output of the function.\n",
    "    n : int\n",
    "        The numbering of the test case.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        assert np.array_equal(arr_result, arr_answer)\n",
    "        print(f\"Test case {n} passed.\")\n",
    "    except:\n",
    "        print(f\"Test case {n} failed.\")\n",
    "        print(\"Your output is:\")\n",
    "        print(arr_result)\n",
    "        print(\"Expected output is:\")\n",
    "        print(arr_answer)\n",
    "        \n",
    "def answer_tokenize_and_lemmatize(df):\n",
    "    \"\"\"\n",
    "    Tokenize and lemmatize the text in the dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe containing at least the text column.\n",
    "         \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The dataframe with the added tokens column.\n",
    "    \"\"\"\n",
    "    # Copy the dataframe to avoid editing the original one.\n",
    "    df = df.copy(deep=True)\n",
    "    \n",
    "    # Add progress bar because lemmatizer can take a while.\n",
    "    tqdm.pandas()\n",
    "\n",
    "    # Apply the tokenizer to create the tokens column.\n",
    "    df['tokens'] = df['text'].progress_apply(word_tokenize)\n",
    "    \n",
    "    # Apply the lemmatizer on every word in the tokens list.\n",
    "    df['tokens'] = df['tokens'].progress_apply(lambda tokens: [lemmatizer.lemmatize(token, wordnet_pos(tag)) for token, tag in nltk.pos_tag(tokens)])\n",
    "    return df\n",
    "\n",
    "\n",
    "def answer_most_used_words(df, token_col='tokens'):\n",
    "    \"\"\"\n",
    "    Generate a dataframe with the 5 most used words per class, and their count.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe containing at least the class and tokens columns.\n",
    "         \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The dataframe with 5 rows per class, and an added 'count' column.\n",
    "        The dataframe is sorted in ascending order on the class and in descending order on the count.\n",
    "    \"\"\"\n",
    "    # Copy the dataframe to avoid editing the original one.\n",
    "    df = df.copy(deep=True)\n",
    "    \n",
    "    # Filter out non-words\n",
    "    df[token_col] = df[token_col].apply(lambda tokens: [token for token in tokens if token.isalpha()])\n",
    "    \n",
    "    # Explode the tokens so that every token gets its own row.\n",
    "    df = df.explode(token_col)\n",
    "    \n",
    "    # Option 1: groupby on class and token, get the size of how many rows per item, \n",
    "    # add that as a column.\n",
    "    counts = df.groupby(['class', token_col]).size().reset_index(name='count')\n",
    "    \n",
    "    # Option 2: make a pivot table based on the class and token based on how many\n",
    "    # rows per combination there are , add counts as a column.\n",
    "    # counts = counts.pivot_table(index=['class', 'tokens'], aggfunc='size').reset_index(name='count')\n",
    "    \n",
    "    # Sort the values on the class and count, get only the first 5 rows per class.\n",
    "    counts = counts.sort_values(['class', 'count'], ascending=[True, False]).groupby('class').head()\n",
    "\n",
    "    return counts\n",
    "\n",
    "def answer_remove_stopwords(df):\n",
    "    \"\"\"\n",
    "    Remove stopwords from the tokens.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe containing at least the tokens column.\n",
    "         \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The dataframe with stopwords removed from the tokens column.\n",
    "    \"\"\"\n",
    "    # Copy the dataframe to avoid editing the original one.\n",
    "    df = df.copy(deep=True)\n",
    "    \n",
    "    # Using a set for quicker lookups.\n",
    "    stopwords_set = set(stopwords_list)\n",
    "    \n",
    "    # Filter stopwords from tokens.\n",
    "    df['tokens'] = df['tokens'].apply(lambda tokens: [token for token in tokens if token.lower() not in stopwords_set])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def answer_spacy_tokens(df):\n",
    "    \"\"\"\n",
    "    Add a column with a list of lemmatized tokens, without stopwords.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe containing at least the doc column.\n",
    "         \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The dataframe with the spacy_tokens column.\n",
    "    \"\"\"\n",
    "    # Copy the dataframe to avoid editing the original one.\n",
    "    df = df.copy(deep=True)\n",
    "    \n",
    "    df['spacy_tokens'] = df['doc'].apply(lambda tokens: [token.lemma_ for token in tokens if not token.is_stop])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def answer_largest_proportion(arr):\n",
    "    \"\"\"\n",
    "    For every row, get the column number where it has the largest value.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : numpy.array\n",
    "        The array with the amount of topics as the amount of columns\n",
    "        and the amount of documents as the number of rows.\n",
    "        Every row should sum up to 1.\n",
    "         \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The 1-dimensional array containing the label of the topic\n",
    "        the document has the largest proportion in.\n",
    "    \"\"\"\n",
    "    return np.argmax(arr, axis=1)\n",
    "\n",
    "def answer_add_padded_tensors(df1, df2):\n",
    "    \"\"\"\n",
    "    Add a tensor column to the dataframes, with every tensor having the same dimensions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_train : pandas.DataFrame\n",
    "        The first dataframe containing at least the tokens or doc columns.\n",
    "    df_test : pandas.DataFrame\n",
    "        The second dataframe containing at least the tokens or doc columns.\n",
    "         \n",
    "    Returns\n",
    "    -------\n",
    "    tuple[pandas.DataFrame]\n",
    "        The dataframes with the added tensor column.\n",
    "    \"\"\"\n",
    "    # Copy the dataframes to avoid editing the originals.\n",
    "    df1 = df1.copy(deep=True)\n",
    "    df2 = df2.copy(deep=True)\n",
    "    \n",
    "    df1 = df1.sample(frac=0.1, random_state=42)\n",
    "    df2 = df2.sample(frac=0.1, random_state=42)\n",
    "    \n",
    "    # Add tensors (option 1: our own model)\n",
    "    for df in [df1, df2]:\n",
    "        df['tensor'] = df['tokens'].apply(lambda tokens: np.vstack([w2v_model.wv[token] for token in tokens]))\n",
    "    \n",
    "    # Add tensors (option 2: spaCy tensors).\n",
    "    for df in [df1, df2]:\n",
    "        df['tensor'] = df['doc'].apply(lambda doc: doc.tensor)\n",
    "    \n",
    "    # Determine the largest amount of columns.\n",
    "    largest = max(df1['tensor'].apply(lambda x: x.shape[0]).max(), \n",
    "                  df2['tensor'].apply(lambda x: x.shape[0]).max())\n",
    "    \n",
    "    # Pad our tensors to that amount.\n",
    "    for df in [df1, df2]:\n",
    "        df['tensor'] = df['tensor'].apply(lambda x: np.pad(x, ((0, largest - x.shape[0]), (0,0))))\n",
    "    \n",
    "    return df1, df2\n",
    "\n",
    "# Confusion matrix code\n",
    "\n",
    "# # Compute the confusion matrix\n",
    "# cm = confusion_matrix(test_labels, test_pred)\n",
    "\n",
    "# # Plot the confusion matrix using seaborn\n",
    "# sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"t3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Preprocess Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will preprocess the text data from the AG News Dataset. First, we need to load the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Index</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>1</td>\n",
       "      <td>Pakistan's Musharraf Says Won't Quit as Army C...</td>\n",
       "      <td>KARACHI (Reuters) - Pakistani President Perve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>2</td>\n",
       "      <td>Renteria signing a top-shelf deal</td>\n",
       "      <td>Red Sox general manager Theo Epstein acknowled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>2</td>\n",
       "      <td>Saban not going to Dolphins yet</td>\n",
       "      <td>The Miami Dolphins will put their courtship of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>2</td>\n",
       "      <td>Today's NFL games</td>\n",
       "      <td>PITTSBURGH at NY GIANTS Time: 1:30 p.m. Line: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>2</td>\n",
       "      <td>Nets get Carter from Raptors</td>\n",
       "      <td>INDIANAPOLIS -- All-Star Vince Carter was trad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Class Index                                              Title  \\\n",
       "0                 3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1                 3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2                 3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3                 3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4                 3  Oil prices soar to all-time record, posing new...   \n",
       "...             ...                                                ...   \n",
       "119995            1  Pakistan's Musharraf Says Won't Quit as Army C...   \n",
       "119996            2                  Renteria signing a top-shelf deal   \n",
       "119997            2                    Saban not going to Dolphins yet   \n",
       "119998            2                                  Today's NFL games   \n",
       "119999            2                       Nets get Carter from Raptors   \n",
       "\n",
       "                                              Description  \n",
       "0       Reuters - Short-sellers, Wall Street's dwindli...  \n",
       "1       Reuters - Private investment firm Carlyle Grou...  \n",
       "2       Reuters - Soaring crude prices plus worries\\ab...  \n",
       "3       Reuters - Authorities have halted oil export\\f...  \n",
       "4       AFP - Tearaway world oil prices, toppling reco...  \n",
       "...                                                   ...  \n",
       "119995   KARACHI (Reuters) - Pakistani President Perve...  \n",
       "119996  Red Sox general manager Theo Epstein acknowled...  \n",
       "119997  The Miami Dolphins will put their courtship of...  \n",
       "119998  PITTSBURGH at NY GIANTS Time: 1:30 p.m. Line: ...  \n",
       "119999  INDIANAPOLIS -- All-Star Vince Carter was trad...  \n",
       "\n",
       "[120000 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Index</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Fears for T N pension after talks</td>\n",
       "      <td>Unions representing workers at Turner   Newall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>The Race is On: Second Private Team Sets Launc...</td>\n",
       "      <td>SPACE.com - TORONTO, Canada -- A second\\team o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Ky. Company Wins Grant to Study Peptides (AP)</td>\n",
       "      <td>AP - A company founded by a chemistry research...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Prediction Unit Helps Forecast Wildfires (AP)</td>\n",
       "      <td>AP - It's barely dawn when Mike Fitzpatrick st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Calif. Aims to Limit Farm-Related Smog (AP)</td>\n",
       "      <td>AP - Southern California's smog-fighting agenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7595</th>\n",
       "      <td>1</td>\n",
       "      <td>Around the world</td>\n",
       "      <td>Ukrainian presidential candidate Viktor Yushch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7596</th>\n",
       "      <td>2</td>\n",
       "      <td>Void is filled with Clement</td>\n",
       "      <td>With the supply of attractive pitching options...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7597</th>\n",
       "      <td>2</td>\n",
       "      <td>Martinez leaves bitter</td>\n",
       "      <td>Like Roger Clemens did almost exactly eight ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7598</th>\n",
       "      <td>3</td>\n",
       "      <td>5 of arthritis patients in Singapore take Bext...</td>\n",
       "      <td>SINGAPORE : Doctors in the United States have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7599</th>\n",
       "      <td>3</td>\n",
       "      <td>EBay gets into rentals</td>\n",
       "      <td>EBay plans to buy the apartment and home renta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7600 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Class Index                                              Title  \\\n",
       "0               3                  Fears for T N pension after talks   \n",
       "1               4  The Race is On: Second Private Team Sets Launc...   \n",
       "2               4      Ky. Company Wins Grant to Study Peptides (AP)   \n",
       "3               4      Prediction Unit Helps Forecast Wildfires (AP)   \n",
       "4               4        Calif. Aims to Limit Farm-Related Smog (AP)   \n",
       "...           ...                                                ...   \n",
       "7595            1                                   Around the world   \n",
       "7596            2                        Void is filled with Clement   \n",
       "7597            2                             Martinez leaves bitter   \n",
       "7598            3  5 of arthritis patients in Singapore take Bext...   \n",
       "7599            3                             EBay gets into rentals   \n",
       "\n",
       "                                            Description  \n",
       "0     Unions representing workers at Turner   Newall...  \n",
       "1     SPACE.com - TORONTO, Canada -- A second\\team o...  \n",
       "2     AP - A company founded by a chemistry research...  \n",
       "3     AP - It's barely dawn when Mike Fitzpatrick st...  \n",
       "4     AP - Southern California's smog-fighting agenc...  \n",
       "...                                                 ...  \n",
       "7595  Ukrainian presidential candidate Viktor Yushch...  \n",
       "7596  With the supply of attractive pitching options...  \n",
       "7597  Like Roger Clemens did almost exactly eight ye...  \n",
       "7598  SINGAPORE : Doctors in the United States have ...  \n",
       "7599  EBay plans to buy the apartment and home renta...  \n",
       "\n",
       "[7600 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "display(df_train, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, all the classes are distributed evenly in the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    30000\n",
       "4    30000\n",
       "2    30000\n",
       "1    30000\n",
       "Name: Class Index, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3    1900\n",
       "4    1900\n",
       "2    1900\n",
       "1    1900\n",
       "Name: Class Index, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_train['Class Index'].value_counts(), df_test['Class Index'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the data more understandable, we will make the classes more understandable by adding a `class` column from the original `Class Index` column, containing the category of the news article. To process both the title and news text together, we will combine the `Title` and `Description` columns into one `text` column. We will just deal with the train data until the point where we need the test data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_idx</th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>1</td>\n",
       "      <td>World</td>\n",
       "      <td>Pakistan's Musharraf Says Won't Quit as Army C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Renteria signing a top-shelf deal Red Sox gene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Saban not going to Dolphins yet The Miami Dolp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Today's NFL games PITTSBURGH at NY GIANTS Time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nets get Carter from Raptors INDIANAPOLIS -- A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        class_idx     class                                               text\n",
       "0               3  Business  Wall St. Bears Claw Back Into the Black (Reute...\n",
       "1               3  Business  Carlyle Looks Toward Commercial Aerospace (Reu...\n",
       "2               3  Business  Oil and Economy Cloud Stocks' Outlook (Reuters...\n",
       "3               3  Business  Iraq Halts Oil Exports from Main Southern Pipe...\n",
       "4               3  Business  Oil prices soar to all-time record, posing new...\n",
       "...           ...       ...                                                ...\n",
       "119995          1     World  Pakistan's Musharraf Says Won't Quit as Army C...\n",
       "119996          2    Sports  Renteria signing a top-shelf deal Red Sox gene...\n",
       "119997          2    Sports  Saban not going to Dolphins yet The Miami Dolp...\n",
       "119998          2    Sports  Today's NFL games PITTSBURGH at NY GIANTS Time...\n",
       "119999          2    Sports  Nets get Carter from Raptors INDIANAPOLIS -- A...\n",
       "\n",
       "[120000 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def reformat_data(df):\n",
    "    \"\"\"\n",
    "    Reformat the Class Index column to a Class column and combine\n",
    "    the Title and Description columns into a Text column.\n",
    "    Select only the class_idx, class and text columns afterwards.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The original dataframe.\n",
    "         \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The reformatted dataframe.\n",
    "    \"\"\"\n",
    "    # Make the class column using a dictionary.\n",
    "    df = df.rename(columns={\"Class Index\": \"class_idx\"})\n",
    "    classes = {1: 'World', 2: 'Sports', 3: 'Business', 4: 'Sci/Tech'}\n",
    "    df['class'] = df['class_idx'].apply(classes.get)\n",
    "    \n",
    "    # Use string concatonation for the Text column and unesacpe html characters.\n",
    "    df['text'] = (df['Title'] + ' ' + df['Description']).apply(su.unescape)\n",
    "    \n",
    "    # Select only the Class and Text columns.\n",
    "    df = df[['class_idx', 'class', 'text']]\n",
    "    return df\n",
    "\n",
    "df_train = reformat_data(df_train)\n",
    "display(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"t3-1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking down a text into individual tokens, which are usually words but can also be phrases or sentences. It helps language models to understand and analyze text data by breaking it down into smaller, more manageable pieces. While it may seem like a trivial task, tokenization can be applied in multiple ways and thus be a complex and challenging task influencing NLP applications.\n",
    "\n",
    "For example, in languages like English, it is generally straightforward to identify words by using spaces as delimiters. However, there are exceptions, such as contractions like \"can't\" and hyphenated words like \"self-driving\". And in Dutch, where multiple nouns can be combined into one bigger noun without any delimiter this can be hard. How would you tokenize \"hippopotomonstrosesquippedaliofobie\"? In other languages, such as Chinese and Japanese, there are no spaces between words, so identifying word boundaries is much more difficult. \n",
    "\n",
    "To illustrate the use of tokenization, let's consider the following example, which tokenizes a sample text using the `word_tokenize` function from the NLTK package. That function uses a pre-trained tokenization model for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: The quick brown fox jumped over the lazy dog. The cats couldn't wait to sleep all day.\n",
      "Tokenized text: ['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', '.', 'The', 'cats', 'could', \"n't\", 'wait', 'to', 'sleep', 'all', 'day', '.']\n"
     ]
    }
   ],
   "source": [
    "# Sample text.\n",
    "text = \"The quick brown fox jumped over the lazy dog. The cats couldn't wait to sleep all day.\"\n",
    "\n",
    "# Tokenize the text.\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Print the text and the tokens.\n",
    "print(\"Original text:\", text)\n",
    "print(\"Tokenized text:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"t3-2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-speech (POS) tagging is the process of assigning each word in a text corpus with a specific part-of-speech tag based on its context and definition. The tags typically include nouns, verbs, adjectives, adverbs, pronouns, preposition, conjunction, interjection, and more. POS tagging can help other NLP tasks disambiguate a token in some way due to the added context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumped', 'VBD'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.'), ('The', 'DT'), ('cats', 'NNS'), ('could', 'MD'), (\"n't\", 'RB'), ('wait', 'VB'), ('to', 'TO'), ('sleep', 'VB'), ('all', 'DT'), ('day', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"t3-3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming / lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and lemmatization are two common techniques used in NLP to preprocess and normalize text data. Both techniques involve transforming words into their root form, but they differ in their approach and the level of normalization they provide.\n",
    "\n",
    "Stemming is a technique that involves reducing words to their base or stem form by removing any affixes or suffixes. For example, the stem of the word \"lazily\" would be \"lazi\". Stemming is a simple and fast technique that can be useful. However, it can also produce inaccurate or incorrect results since it does not consider the context or part of speech of the word.\n",
    "\n",
    "Lemmatization, on the other hand, is a more sophisticated technique that involves identifying the base or dictionary form of a word, also known as the lemma. Unlike stemming, lemmatization can consider the context and part of speech of the word, which can make it more accurate and reliable. With lemmatization, the lemma of the word \"lazily\" would be \"lazy\". Lemmatization can be slower and more complex than stemming but provides a higher level of normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed text: ['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog', '.', 'the', 'cat', 'could', \"n't\", 'wait', 'to', 'sleep', 'all', 'day', '.']\n",
      "Lemmatized text: ['The', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazy', 'dog', '.', 'The', 'cat', 'could', \"n't\", 'wait', 'to', 'sleep', 'all', 'day', '.']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the stemmer and lemmatizer.\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def wordnet_pos(nltk_pos):\n",
    "    \"\"\"\n",
    "    Function to map POS tags to wordnet tags for lemmatizer.\n",
    "    \"\"\"\n",
    "    if nltk_pos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_pos.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_pos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    return wordnet.NOUN\n",
    "\n",
    "# Perform stemming and lemmatization seperately on the tokens.\n",
    "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token, wordnet_pos(tag)) for token, tag in nltk.pos_tag(tokens)]\n",
    "\n",
    "# Print the results.\n",
    "print(\"Stemmed text:\", stemmed_tokens)\n",
    "print(\"Lemmatized text:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"t3-4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopword removal is a common technique used in NLP to preprocess and clean text data by removing words that are considered to be of little or no value in terms of conveying meaning or information. These words are called \"stopwords\" and they include common words such as \"the\", \"a\", \"an\", \"and\", \"or\", \"but\", and so on.\n",
    "\n",
    "The purpose of stopword removal in NLP is to improve the accuracy and efficiency of text analysis and processing by reducing the noise and complexity of the data. Stopwords are often used to form grammatical structures in a sentence, but they do not carry much meaning or relevance to the main topic or theme of the text. So by removing these words, we can reduce the dimensionality of the text data, improve the performance of machine learning models, and speed up the processing of text data. NLTK has a predefined list of stopwords for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# English stopwords in NLTK.\n",
    "stopwords_list = stopwords.words('english')\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"a3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment for Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your task (which is your assignment) is to write functions to do the following:**\n",
    "- Since we want to use our text to make a model later on, we need to preprocess it. Add a `tokens` column to the `df_train` dataframe with the text tokenized, then lemmatize those tokens. You must use the POS tags when lemmatizing.\n",
    "    - Hint: Use the `pandas.Series.apply` function with the imported `nltk.tokenize.word_tokenize` function. Recall that you can use the `pd.Series.apply?` syntax in a code cell for more information.\n",
    "    - Hint: use the `nltk.stem.WordNetLemmatizer.lemmatize` function to lemmatize a token. Use the `wordnet_pos` function to obtain the POS tag for the lemmatizer. \n",
    "    \n",
    "    Tokenizing and lemmatizing the entire dataset can take a while too. We use `tqdm` and the `pandas.Series.progress_apply` in the answer version to show progress bars for the operations.\n",
    "\n",
    "    Our goal is to have a dataframe that looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e629f3f61f4a3990be4b28891f23b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "675881b947b94d84ad086f2cfc21c0f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_idx</th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
       "      <td>[Wall, St., Bears, Claw, Back, Into, the, Blac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>[Carlyle, Looks, Toward, Commercial, Aerospace...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "      <td>[Oil, and, Economy, Cloud, Stocks, ', Outlook,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>[Iraq, Halts, Oil, Exports, from, Main, Southe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>[Oil, price, soar, to, all-time, record, ,, po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>1</td>\n",
       "      <td>World</td>\n",
       "      <td>Pakistan's Musharraf Says Won't Quit as Army C...</td>\n",
       "      <td>[Pakistan, 's, Musharraf, Says, Wo, n't, Quit,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Renteria signing a top-shelf deal Red Sox gene...</td>\n",
       "      <td>[Renteria, sign, a, top-shelf, deal, Red, Sox,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Saban not going to Dolphins yet The Miami Dolp...</td>\n",
       "      <td>[Saban, not, go, to, Dolphins, yet, The, Miami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Today's NFL games PITTSBURGH at NY GIANTS Time...</td>\n",
       "      <td>[Today, 's, NFL, game, PITTSBURGH, at, NY, GIA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nets get Carter from Raptors INDIANAPOLIS -- A...</td>\n",
       "      <td>[Nets, get, Carter, from, Raptors, INDIANAPOLI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        class_idx     class  \\\n",
       "0               3  Business   \n",
       "1               3  Business   \n",
       "2               3  Business   \n",
       "3               3  Business   \n",
       "4               3  Business   \n",
       "...           ...       ...   \n",
       "119995          1     World   \n",
       "119996          2    Sports   \n",
       "119997          2    Sports   \n",
       "119998          2    Sports   \n",
       "119999          2    Sports   \n",
       "\n",
       "                                                     text  \\\n",
       "0       Wall St. Bears Claw Back Into the Black (Reute...   \n",
       "1       Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2       Oil and Economy Cloud Stocks' Outlook (Reuters...   \n",
       "3       Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4       Oil prices soar to all-time record, posing new...   \n",
       "...                                                   ...   \n",
       "119995  Pakistan's Musharraf Says Won't Quit as Army C...   \n",
       "119996  Renteria signing a top-shelf deal Red Sox gene...   \n",
       "119997  Saban not going to Dolphins yet The Miami Dolp...   \n",
       "119998  Today's NFL games PITTSBURGH at NY GIANTS Time...   \n",
       "119999  Nets get Carter from Raptors INDIANAPOLIS -- A...   \n",
       "\n",
       "                                                   tokens  \n",
       "0       [Wall, St., Bears, Claw, Back, Into, the, Blac...  \n",
       "1       [Carlyle, Looks, Toward, Commercial, Aerospace...  \n",
       "2       [Oil, and, Economy, Cloud, Stocks, ', Outlook,...  \n",
       "3       [Iraq, Halts, Oil, Exports, from, Main, Southe...  \n",
       "4       [Oil, price, soar, to, all-time, record, ,, po...  \n",
       "...                                                   ...  \n",
       "119995  [Pakistan, 's, Musharraf, Says, Wo, n't, Quit,...  \n",
       "119996  [Renteria, sign, a, top-shelf, deal, Red, Sox,...  \n",
       "119997  [Saban, not, go, to, Dolphins, yet, The, Miami...  \n",
       "119998  [Today, 's, NFL, game, PITTSBURGH, at, NY, GIA...  \n",
       "119999  [Nets, get, Carter, from, Raptors, INDIANAPOLI...  \n",
       "\n",
       "[120000 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This part of code will take several minutes to run.\n",
    "answer_df = answer_tokenize_and_lemmatize(df_train)\n",
    "display(answer_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To see what the most used words per class are, create a new, seperate dataframe with the 5 most used words per class. Sort the resulting dataframe ascending on the `class` and descending on the `count`.\n",
    "    - Hint: use the `pandas.Series.apply` and `str.isalpha()` functions to filter out non-alphabetical tokens.\n",
    "    - Hint: use the `pandas.DataFrame.explode` to create one row per class and token.\n",
    "    - Hint: use `pandas.DataFrame.groupby` with `.size()` afterwards or `pandas.DataFrame.pivot_table` with `size` as the `aggfunc` to obtain the occurences per class.\n",
    "    - Hint: use the `pandas.Series.reset_index` function to obtain a dataframe with `[class, tokens, count]` as the columns.\n",
    "    - Hint: use the `pandas.DataFrame.sort_values` function for sorting a dataframe.\n",
    "    - Hint: use the `pandas.DataFrame.groupby` and `pandas.DataFrame.head` functions to get the first 5 rows per class.\n",
    "        \n",
    "    Our goal is to have a dataframe that looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tokens</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28111</th>\n",
       "      <td>Business</td>\n",
       "      <td>the</td>\n",
       "      <td>37998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17122</th>\n",
       "      <td>Business</td>\n",
       "      <td>a</td>\n",
       "      <td>30841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28259</th>\n",
       "      <td>Business</td>\n",
       "      <td>to</td>\n",
       "      <td>29384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24422</th>\n",
       "      <td>Business</td>\n",
       "      <td>of</td>\n",
       "      <td>22539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22506</th>\n",
       "      <td>Business</td>\n",
       "      <td>in</td>\n",
       "      <td>21446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63963</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>the</td>\n",
       "      <td>40767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64137</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>to</td>\n",
       "      <td>30497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49851</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>a</td>\n",
       "      <td>27686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59243</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>of</td>\n",
       "      <td>26048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50993</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>be</td>\n",
       "      <td>19238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97908</th>\n",
       "      <td>Sports</td>\n",
       "      <td>the</td>\n",
       "      <td>56416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86396</th>\n",
       "      <td>Sports</td>\n",
       "      <td>a</td>\n",
       "      <td>29398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98078</th>\n",
       "      <td>Sports</td>\n",
       "      <td>to</td>\n",
       "      <td>27171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92067</th>\n",
       "      <td>Sports</td>\n",
       "      <td>in</td>\n",
       "      <td>23187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93981</th>\n",
       "      <td>Sports</td>\n",
       "      <td>of</td>\n",
       "      <td>20119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130423</th>\n",
       "      <td>World</td>\n",
       "      <td>the</td>\n",
       "      <td>42548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117945</th>\n",
       "      <td>World</td>\n",
       "      <td>a</td>\n",
       "      <td>32397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124106</th>\n",
       "      <td>World</td>\n",
       "      <td>in</td>\n",
       "      <td>31243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130599</th>\n",
       "      <td>World</td>\n",
       "      <td>to</td>\n",
       "      <td>30663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126243</th>\n",
       "      <td>World</td>\n",
       "      <td>of</td>\n",
       "      <td>28728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           class tokens  count\n",
       "28111   Business    the  37998\n",
       "17122   Business      a  30841\n",
       "28259   Business     to  29384\n",
       "24422   Business     of  22539\n",
       "22506   Business     in  21446\n",
       "63963   Sci/Tech    the  40767\n",
       "64137   Sci/Tech     to  30497\n",
       "49851   Sci/Tech      a  27686\n",
       "59243   Sci/Tech     of  26048\n",
       "50993   Sci/Tech     be  19238\n",
       "97908     Sports    the  56416\n",
       "86396     Sports      a  29398\n",
       "98078     Sports     to  27171\n",
       "92067     Sports     in  23187\n",
       "93981     Sports     of  20119\n",
       "130423     World    the  42548\n",
       "117945     World      a  32397\n",
       "124106     World     in  31243\n",
       "130599     World     to  30663\n",
       "126243     World     of  28728"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(answer_most_used_words(answer_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove the stopwords from the `tokens` column in the `df_train` dataframe. Do the most used tokens say something about the class now?\n",
    "    - Hint: once again, you can use the `pandas.Series.apply` function. \n",
    "    \n",
    "    The top 5 words per class should look like this after removing stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tokens</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26317</th>\n",
       "      <td>Business</td>\n",
       "      <td>say</td>\n",
       "      <td>8879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12756</th>\n",
       "      <td>Business</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>6893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15719</th>\n",
       "      <td>Business</td>\n",
       "      <td>US</td>\n",
       "      <td>5609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18895</th>\n",
       "      <td>Business</td>\n",
       "      <td>company</td>\n",
       "      <td>5062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25086</th>\n",
       "      <td>Business</td>\n",
       "      <td>price</td>\n",
       "      <td>4611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61365</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>say</td>\n",
       "      <td>5536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58378</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>new</td>\n",
       "      <td>5149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40328</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>5041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51846</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>company</td>\n",
       "      <td>3781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29300</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>AP</td>\n",
       "      <td>3682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65192</th>\n",
       "      <td>Sports</td>\n",
       "      <td>AP</td>\n",
       "      <td>6245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98155</th>\n",
       "      <td>Sports</td>\n",
       "      <td>win</td>\n",
       "      <td>5492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90163</th>\n",
       "      <td>Sports</td>\n",
       "      <td>game</td>\n",
       "      <td>3819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89768</th>\n",
       "      <td>Sports</td>\n",
       "      <td>first</td>\n",
       "      <td>3696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96792</th>\n",
       "      <td>Sports</td>\n",
       "      <td>team</td>\n",
       "      <td>3571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127374</th>\n",
       "      <td>World</td>\n",
       "      <td>say</td>\n",
       "      <td>10299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106496</th>\n",
       "      <td>World</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>5760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98469</th>\n",
       "      <td>World</td>\n",
       "      <td>AP</td>\n",
       "      <td>5757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112273</th>\n",
       "      <td>World</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>5406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123453</th>\n",
       "      <td>World</td>\n",
       "      <td>kill</td>\n",
       "      <td>4545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           class     tokens  count\n",
       "26317   Business        say   8879\n",
       "12756   Business    Reuters   6893\n",
       "15719   Business         US   5609\n",
       "18895   Business    company   5062\n",
       "25086   Business      price   4611\n",
       "61365   Sci/Tech        say   5536\n",
       "58378   Sci/Tech        new   5149\n",
       "40328   Sci/Tech  Microsoft   5041\n",
       "51846   Sci/Tech    company   3781\n",
       "29300   Sci/Tech         AP   3682\n",
       "65192     Sports         AP   6245\n",
       "98155     Sports        win   5492\n",
       "90163     Sports       game   3819\n",
       "89768     Sports      first   3696\n",
       "96792     Sports       team   3571\n",
       "127374     World        say  10299\n",
       "106496     World       Iraq   5760\n",
       "98469      World         AP   5757\n",
       "112273     World    Reuters   5406\n",
       "123453     World       kill   4545"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer_df = answer_remove_stopwords(answer_df)\n",
    "display(answer_most_used_words(answer_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_lemmatize(df):\n",
    "    \"\"\"\n",
    "    Tokenize and lemmatize the text in the dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe containing at least the text column.\n",
    "         \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The dataframe with the added tokens column.\n",
    "    \"\"\"\n",
    "    ###################################\n",
    "    # Fill in your answer here\n",
    "    return None\n",
    "    ###################################\n",
    "\n",
    "\n",
    "def most_used_words(df, token_col='tokens'):\n",
    "    \"\"\"\n",
    "    Generate a dataframe with the 5 most used words per class, and their count.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe containing at least the class and tokens columns.\n",
    "         \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The dataframe with 5 rows per class, and an added 'count' column.\n",
    "        The dataframe is sorted in ascending order on the class and in descending order on the count.\n",
    "    \"\"\"\n",
    "    ###################################\n",
    "    # Fill in your answer here\n",
    "    return None\n",
    "    ###################################\n",
    "\n",
    "def remove_stopwords(df):\n",
    "    \"\"\"\n",
    "    Remove stopwords from the tokens.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe containing at least the tokens column.\n",
    "         \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The dataframe with stopwords removed from the tokens column.\n",
    "    \"\"\"\n",
    "    ###################################\n",
    "    # Fill in your answer here\n",
    "    return None\n",
    "    ###################################\n",
    "    \n",
    "df_train = df_train  # Edit this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below tests if all your functions combined match the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test case 1 failed.\n",
      "Your output is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected output is:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tokens</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26317</th>\n",
       "      <td>Business</td>\n",
       "      <td>say</td>\n",
       "      <td>8879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12756</th>\n",
       "      <td>Business</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>6893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15719</th>\n",
       "      <td>Business</td>\n",
       "      <td>US</td>\n",
       "      <td>5609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18895</th>\n",
       "      <td>Business</td>\n",
       "      <td>company</td>\n",
       "      <td>5062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25086</th>\n",
       "      <td>Business</td>\n",
       "      <td>price</td>\n",
       "      <td>4611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61365</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>say</td>\n",
       "      <td>5536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58378</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>new</td>\n",
       "      <td>5149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40328</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>5041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51846</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>company</td>\n",
       "      <td>3781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29300</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>AP</td>\n",
       "      <td>3682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65192</th>\n",
       "      <td>Sports</td>\n",
       "      <td>AP</td>\n",
       "      <td>6245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98155</th>\n",
       "      <td>Sports</td>\n",
       "      <td>win</td>\n",
       "      <td>5492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90163</th>\n",
       "      <td>Sports</td>\n",
       "      <td>game</td>\n",
       "      <td>3819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89768</th>\n",
       "      <td>Sports</td>\n",
       "      <td>first</td>\n",
       "      <td>3696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96792</th>\n",
       "      <td>Sports</td>\n",
       "      <td>team</td>\n",
       "      <td>3571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127374</th>\n",
       "      <td>World</td>\n",
       "      <td>say</td>\n",
       "      <td>10299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106496</th>\n",
       "      <td>World</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>5760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98469</th>\n",
       "      <td>World</td>\n",
       "      <td>AP</td>\n",
       "      <td>5757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112273</th>\n",
       "      <td>World</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>5406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123453</th>\n",
       "      <td>World</td>\n",
       "      <td>kill</td>\n",
       "      <td>4545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           class     tokens  count\n",
       "26317   Business        say   8879\n",
       "12756   Business    Reuters   6893\n",
       "15719   Business         US   5609\n",
       "18895   Business    company   5062\n",
       "25086   Business      price   4611\n",
       "61365   Sci/Tech        say   5536\n",
       "58378   Sci/Tech        new   5149\n",
       "40328   Sci/Tech  Microsoft   5041\n",
       "51846   Sci/Tech    company   3781\n",
       "29300   Sci/Tech         AP   3682\n",
       "65192     Sports         AP   6245\n",
       "98155     Sports        win   5492\n",
       "90163     Sports       game   3819\n",
       "89768     Sports      first   3696\n",
       "96792     Sports       team   3571\n",
       "127374     World        say  10299\n",
       "106496     World       Iraq   5760\n",
       "98469      World         AP   5757\n",
       "112273     World    Reuters   5406\n",
       "123453     World       kill   4545"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_answer_df(most_used_words(df_train), answer_most_used_words(answer_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"t4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Another option: spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy is another library used to perform various NLP tasks like tokenization, part-of-speech tagging, named entity recognition, dependency parsing, and much more. It provides pre-trained models for different languages and domains, which can be used as-is but also can be fine-tuned on a specific task or domain.\n",
    "\n",
    "In an object-oriented way, spaCy can be thought of as a collection of classes and objects that work together to perform NLP tasks. Some of the important functions and classes in spaCy include:\n",
    "\n",
    "- `nlp`: The core function that provides the main functionality of spaCy. It is used to process text and create a `Doc` object.\n",
    "- [`Doc`](https://spacy.io/api/doc): A container for accessing linguistic annotations like tokens, part-of-speech tags, named entities, and dependency parse information. It is created by the `nlp` function and represents a processed document.\n",
    "- [`Token`](https://spacy.io/api/token): An object representing a single token in a `Doc` object. It contains information like the token text, part-of-speech tag, lemma, embedding, and much more.\n",
    "\n",
    "When a text is processed by spaCy, it is first passed to the nlp function, which uses the loaded model to tokenize the text and applies various linguistic annotations like part-of-speech tagging, named entity recognition, and dependency parsing in the background. The resulting annotations are stored in a Doc object, which can be accessed and manipulated using various methods and attributes. For example, the Doc object can be iterated over to access each Token object in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'> The quick brown fox jumped over the lazy dog. The cats couldn't wait to sleep all day.\n",
      "<class 'spacy.tokens.token.Token'>\n",
      "The\n",
      "quick\n",
      "brown\n",
      "fox\n",
      "jumped\n",
      "over\n",
      "the\n",
      "lazy\n",
      "dog\n",
      ".\n",
      "The\n",
      "cats\n",
      "could\n",
      "n't\n",
      "wait\n",
      "to\n",
      "sleep\n",
      "all\n",
      "day\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Load the small English model in spaCy.\n",
    "# Disable Named Entity Recognition and the parser in the model pipeline since we're not using it.\n",
    "# Check the following website for the spaCy NLP pipeline:\n",
    "# - https://spacy.io/usage/processing-pipelines\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# Process the text using spaCy.\n",
    "doc = nlp(text)\n",
    "\n",
    "# This becomes a spaCy Doc object, which prints nicely as the original string.\n",
    "print(type(doc) , doc)\n",
    "\n",
    "# We can iterate over the tokens in the Doc, since it has already been tokenized underneath.\n",
    "print(type(doc[0]))\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a lot of processing has already been done, we can also directly access multiple attributes of the `Token` objects. For example, we can directly access the lemma of the token with `Token.lemma_` and check if a token is a stop word with `Token.is_stop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the <class 'str'> True <class 'bool'>\n"
     ]
    }
   ],
   "source": [
    "print(doc[0].lemma_, type(doc[0].lemma_), doc[0].is_stop, type(doc[0].is_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code to add a column with a `Doc` representation of the `text` column to the dataframe. Executing this cell takes several minutes, so we added a progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d78a89939e2340bf90d5a0efbc887e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_idx</th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
       "      <td>[Wall, St., Bears, Claw, Back, Black, (, Reute...</td>\n",
       "      <td>(Wall, St., Bears, Claw, Back, Into, the, Blac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>[Carlyle, Looks, Toward, Commercial, Aerospace...</td>\n",
       "      <td>(Carlyle, Looks, Toward, Commercial, Aerospace...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "      <td>[Oil, Economy, Cloud, Stocks, ', Outlook, (, R...</td>\n",
       "      <td>(Oil, and, Economy, Cloud, Stocks, ', Outlook,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>[Iraq, Halts, Oil, Exports, Main, Southern, Pi...</td>\n",
       "      <td>(Iraq, Halts, Oil, Exports, from, Main, Southe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>[Oil, price, soar, all-time, record, ,, pose, ...</td>\n",
       "      <td>(Oil, prices, soar, to, all, -, time, record, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>1</td>\n",
       "      <td>World</td>\n",
       "      <td>Pakistan's Musharraf Says Won't Quit as Army C...</td>\n",
       "      <td>[Pakistan, 's, Musharraf, Says, Wo, n't, Quit,...</td>\n",
       "      <td>(Pakistan, 's, Musharraf, Says, Wo, n't, Quit,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Renteria signing a top-shelf deal Red Sox gene...</td>\n",
       "      <td>[Renteria, sign, top-shelf, deal, Red, Sox, ge...</td>\n",
       "      <td>(Renteria, signing, a, top, -, shelf, deal, Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Saban not going to Dolphins yet The Miami Dolp...</td>\n",
       "      <td>[Saban, go, Dolphins, yet, Miami, Dolphins, pu...</td>\n",
       "      <td>(Saban, not, going, to, Dolphins, yet, The, Mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Today's NFL games PITTSBURGH at NY GIANTS Time...</td>\n",
       "      <td>[Today, 's, NFL, game, PITTSBURGH, NY, GIANTS,...</td>\n",
       "      <td>(Today, 's, NFL, games, PITTSBURGH, at, NY, GI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nets get Carter from Raptors INDIANAPOLIS -- A...</td>\n",
       "      <td>[Nets, get, Carter, Raptors, INDIANAPOLIS, --,...</td>\n",
       "      <td>(Nets, get, Carter, from, Raptors, INDIANAPOLI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        class_idx     class  \\\n",
       "0               3  Business   \n",
       "1               3  Business   \n",
       "2               3  Business   \n",
       "3               3  Business   \n",
       "4               3  Business   \n",
       "...           ...       ...   \n",
       "119995          1     World   \n",
       "119996          2    Sports   \n",
       "119997          2    Sports   \n",
       "119998          2    Sports   \n",
       "119999          2    Sports   \n",
       "\n",
       "                                                     text  \\\n",
       "0       Wall St. Bears Claw Back Into the Black (Reute...   \n",
       "1       Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2       Oil and Economy Cloud Stocks' Outlook (Reuters...   \n",
       "3       Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4       Oil prices soar to all-time record, posing new...   \n",
       "...                                                   ...   \n",
       "119995  Pakistan's Musharraf Says Won't Quit as Army C...   \n",
       "119996  Renteria signing a top-shelf deal Red Sox gene...   \n",
       "119997  Saban not going to Dolphins yet The Miami Dolp...   \n",
       "119998  Today's NFL games PITTSBURGH at NY GIANTS Time...   \n",
       "119999  Nets get Carter from Raptors INDIANAPOLIS -- A...   \n",
       "\n",
       "                                                   tokens  \\\n",
       "0       [Wall, St., Bears, Claw, Back, Black, (, Reute...   \n",
       "1       [Carlyle, Looks, Toward, Commercial, Aerospace...   \n",
       "2       [Oil, Economy, Cloud, Stocks, ', Outlook, (, R...   \n",
       "3       [Iraq, Halts, Oil, Exports, Main, Southern, Pi...   \n",
       "4       [Oil, price, soar, all-time, record, ,, pose, ...   \n",
       "...                                                   ...   \n",
       "119995  [Pakistan, 's, Musharraf, Says, Wo, n't, Quit,...   \n",
       "119996  [Renteria, sign, top-shelf, deal, Red, Sox, ge...   \n",
       "119997  [Saban, go, Dolphins, yet, Miami, Dolphins, pu...   \n",
       "119998  [Today, 's, NFL, game, PITTSBURGH, NY, GIANTS,...   \n",
       "119999  [Nets, get, Carter, Raptors, INDIANAPOLIS, --,...   \n",
       "\n",
       "                                                      doc  \n",
       "0       (Wall, St., Bears, Claw, Back, Into, the, Blac...  \n",
       "1       (Carlyle, Looks, Toward, Commercial, Aerospace...  \n",
       "2       (Oil, and, Economy, Cloud, Stocks, ', Outlook,...  \n",
       "3       (Iraq, Halts, Oil, Exports, from, Main, Southe...  \n",
       "4       (Oil, prices, soar, to, all, -, time, record, ...  \n",
       "...                                                   ...  \n",
       "119995  (Pakistan, 's, Musharraf, Says, Wo, n't, Quit,...  \n",
       "119996  (Renteria, signing, a, top, -, shelf, deal, Re...  \n",
       "119997  (Saban, not, going, to, Dolphins, yet, The, Mi...  \n",
       "119998  (Today, 's, NFL, games, PITTSBURGH, at, NY, GI...  \n",
       "119999  (Nets, get, Carter, from, Raptors, INDIANAPOLI...  \n",
       "\n",
       "[120000 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_spacy(df):\n",
    "    \"\"\"\n",
    "    Add a column with the spaCy Doc objects.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe containing at least the text column.\n",
    "         \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The dataframe with the added doc column.\n",
    "    \"\"\"\n",
    "    # Copy the dataframe to avoid editing the original one.\n",
    "    df = df.copy(deep=True)\n",
    "    \n",
    "    # Get the number of CPUs in the machine.\n",
    "    n_process = max(1, os.cpu_count()-2)\n",
    "    \n",
    "    # Use multiple CPUs to speed up computing.\n",
    "    df['doc'] = [doc for doc in tqdm(nlp.pipe(df['text'], n_process=n_process), total=df.shape[0])]\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_train = add_spacy(answer_df)\n",
    "display(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"a4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment for Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your task (which is your assignment) is to write a function to do the following:**\n",
    "- Add a `spacy_tokens` column containing the to the `df_train` dataframe containing a list of lemmatized tokens (strings).\n",
    "    \n",
    "    Our goal is to have a dataframe that looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_idx</th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>doc</th>\n",
       "      <th>spacy_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
       "      <td>[Wall, St., Bears, Claw, Back, Black, (, Reute...</td>\n",
       "      <td>(Wall, St., Bears, Claw, Back, Into, the, Blac...</td>\n",
       "      <td>[Wall, St., Bears, Claw, Black, (, Reuters, ),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>[Carlyle, Looks, Toward, Commercial, Aerospace...</td>\n",
       "      <td>(Carlyle, Looks, Toward, Commercial, Aerospace...</td>\n",
       "      <td>[Carlyle, look, Commercial, Aerospace, (, Reut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "      <td>[Oil, Economy, Cloud, Stocks, ', Outlook, (, R...</td>\n",
       "      <td>(Oil, and, Economy, Cloud, Stocks, ', Outlook,...</td>\n",
       "      <td>[oil, Economy, Cloud, Stocks, ', Outlook, (, R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>[Iraq, Halts, Oil, Exports, Main, Southern, Pi...</td>\n",
       "      <td>(Iraq, Halts, Oil, Exports, from, Main, Southe...</td>\n",
       "      <td>[Iraq, Halts, Oil, Exports, Main, Southern, Pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>[Oil, price, soar, all-time, record, ,, pose, ...</td>\n",
       "      <td>(Oil, prices, soar, to, all, -, time, record, ...</td>\n",
       "      <td>[oil, price, soar, -, time, record, ,, pose, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>1</td>\n",
       "      <td>World</td>\n",
       "      <td>Pakistan's Musharraf Says Won't Quit as Army C...</td>\n",
       "      <td>[Pakistan, 's, Musharraf, Says, Wo, n't, Quit,...</td>\n",
       "      <td>(Pakistan, 's, Musharraf, Says, Wo, n't, Quit,...</td>\n",
       "      <td>[Pakistan, Musharraf, say, will, Quit, Army, C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Renteria signing a top-shelf deal Red Sox gene...</td>\n",
       "      <td>[Renteria, sign, top-shelf, deal, Red, Sox, ge...</td>\n",
       "      <td>(Renteria, signing, a, top, -, shelf, deal, Re...</td>\n",
       "      <td>[Renteria, sign, -, shelf, deal, Red, Sox, gen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Saban not going to Dolphins yet The Miami Dolp...</td>\n",
       "      <td>[Saban, go, Dolphins, yet, Miami, Dolphins, pu...</td>\n",
       "      <td>(Saban, not, going, to, Dolphins, yet, The, Mi...</td>\n",
       "      <td>[saban, go, Dolphins, Miami, Dolphins, courtsh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Today's NFL games PITTSBURGH at NY GIANTS Time...</td>\n",
       "      <td>[Today, 's, NFL, game, PITTSBURGH, NY, GIANTS,...</td>\n",
       "      <td>(Today, 's, NFL, games, PITTSBURGH, at, NY, GI...</td>\n",
       "      <td>[today, NFL, game, pittsburgh, NY, giant, time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nets get Carter from Raptors INDIANAPOLIS -- A...</td>\n",
       "      <td>[Nets, get, Carter, Raptors, INDIANAPOLIS, --,...</td>\n",
       "      <td>(Nets, get, Carter, from, Raptors, INDIANAPOLI...</td>\n",
       "      <td>[net, Carter, Raptors, INDIANAPOLIS, --, -, St...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        class_idx     class  \\\n",
       "0               3  Business   \n",
       "1               3  Business   \n",
       "2               3  Business   \n",
       "3               3  Business   \n",
       "4               3  Business   \n",
       "...           ...       ...   \n",
       "119995          1     World   \n",
       "119996          2    Sports   \n",
       "119997          2    Sports   \n",
       "119998          2    Sports   \n",
       "119999          2    Sports   \n",
       "\n",
       "                                                     text  \\\n",
       "0       Wall St. Bears Claw Back Into the Black (Reute...   \n",
       "1       Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2       Oil and Economy Cloud Stocks' Outlook (Reuters...   \n",
       "3       Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4       Oil prices soar to all-time record, posing new...   \n",
       "...                                                   ...   \n",
       "119995  Pakistan's Musharraf Says Won't Quit as Army C...   \n",
       "119996  Renteria signing a top-shelf deal Red Sox gene...   \n",
       "119997  Saban not going to Dolphins yet The Miami Dolp...   \n",
       "119998  Today's NFL games PITTSBURGH at NY GIANTS Time...   \n",
       "119999  Nets get Carter from Raptors INDIANAPOLIS -- A...   \n",
       "\n",
       "                                                   tokens  \\\n",
       "0       [Wall, St., Bears, Claw, Back, Black, (, Reute...   \n",
       "1       [Carlyle, Looks, Toward, Commercial, Aerospace...   \n",
       "2       [Oil, Economy, Cloud, Stocks, ', Outlook, (, R...   \n",
       "3       [Iraq, Halts, Oil, Exports, Main, Southern, Pi...   \n",
       "4       [Oil, price, soar, all-time, record, ,, pose, ...   \n",
       "...                                                   ...   \n",
       "119995  [Pakistan, 's, Musharraf, Says, Wo, n't, Quit,...   \n",
       "119996  [Renteria, sign, top-shelf, deal, Red, Sox, ge...   \n",
       "119997  [Saban, go, Dolphins, yet, Miami, Dolphins, pu...   \n",
       "119998  [Today, 's, NFL, game, PITTSBURGH, NY, GIANTS,...   \n",
       "119999  [Nets, get, Carter, Raptors, INDIANAPOLIS, --,...   \n",
       "\n",
       "                                                      doc  \\\n",
       "0       (Wall, St., Bears, Claw, Back, Into, the, Blac...   \n",
       "1       (Carlyle, Looks, Toward, Commercial, Aerospace...   \n",
       "2       (Oil, and, Economy, Cloud, Stocks, ', Outlook,...   \n",
       "3       (Iraq, Halts, Oil, Exports, from, Main, Southe...   \n",
       "4       (Oil, prices, soar, to, all, -, time, record, ...   \n",
       "...                                                   ...   \n",
       "119995  (Pakistan, 's, Musharraf, Says, Wo, n't, Quit,...   \n",
       "119996  (Renteria, signing, a, top, -, shelf, deal, Re...   \n",
       "119997  (Saban, not, going, to, Dolphins, yet, The, Mi...   \n",
       "119998  (Today, 's, NFL, games, PITTSBURGH, at, NY, GI...   \n",
       "119999  (Nets, get, Carter, from, Raptors, INDIANAPOLI...   \n",
       "\n",
       "                                             spacy_tokens  \n",
       "0       [Wall, St., Bears, Claw, Black, (, Reuters, ),...  \n",
       "1       [Carlyle, look, Commercial, Aerospace, (, Reut...  \n",
       "2       [oil, Economy, Cloud, Stocks, ', Outlook, (, R...  \n",
       "3       [Iraq, Halts, Oil, Exports, Main, Southern, Pi...  \n",
       "4       [oil, price, soar, -, time, record, ,, pose, n...  \n",
       "...                                                   ...  \n",
       "119995  [Pakistan, Musharraf, say, will, Quit, Army, C...  \n",
       "119996  [Renteria, sign, -, shelf, deal, Red, Sox, gen...  \n",
       "119997  [saban, go, Dolphins, Miami, Dolphins, courtsh...  \n",
       "119998  [today, NFL, game, pittsburgh, NY, giant, time...  \n",
       "119999  [net, Carter, Raptors, INDIANAPOLIS, --, -, St...  \n",
       "\n",
       "[120000 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer_df = answer_spacy_tokens(df_train)\n",
    "display(answer_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokens(df):\n",
    "    \"\"\"\n",
    "    Add a column with a list of lemmatized tokens, without stopwords.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe containing at least the doc column.\n",
    "         \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The dataframe with the spacy_tokens column.\n",
    "    \"\"\"\n",
    "    ###################################\n",
    "    # Fill in your answer here\n",
    "    return None\n",
    "    ###################################\n",
    "\n",
    "df_train = df_train  # Edit this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below tests if the function matches the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test case 1 failed.\n",
      "Your output is:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_idx</th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
       "      <td>[Wall, St., Bears, Claw, Back, Black, (, Reute...</td>\n",
       "      <td>(Wall, St., Bears, Claw, Back, Into, the, Blac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>[Carlyle, Looks, Toward, Commercial, Aerospace...</td>\n",
       "      <td>(Carlyle, Looks, Toward, Commercial, Aerospace...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "      <td>[Oil, Economy, Cloud, Stocks, ', Outlook, (, R...</td>\n",
       "      <td>(Oil, and, Economy, Cloud, Stocks, ', Outlook,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>[Iraq, Halts, Oil, Exports, Main, Southern, Pi...</td>\n",
       "      <td>(Iraq, Halts, Oil, Exports, from, Main, Southe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>[Oil, price, soar, all-time, record, ,, pose, ...</td>\n",
       "      <td>(Oil, prices, soar, to, all, -, time, record, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>1</td>\n",
       "      <td>World</td>\n",
       "      <td>Pakistan's Musharraf Says Won't Quit as Army C...</td>\n",
       "      <td>[Pakistan, 's, Musharraf, Says, Wo, n't, Quit,...</td>\n",
       "      <td>(Pakistan, 's, Musharraf, Says, Wo, n't, Quit,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Renteria signing a top-shelf deal Red Sox gene...</td>\n",
       "      <td>[Renteria, sign, top-shelf, deal, Red, Sox, ge...</td>\n",
       "      <td>(Renteria, signing, a, top, -, shelf, deal, Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Saban not going to Dolphins yet The Miami Dolp...</td>\n",
       "      <td>[Saban, go, Dolphins, yet, Miami, Dolphins, pu...</td>\n",
       "      <td>(Saban, not, going, to, Dolphins, yet, The, Mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Today's NFL games PITTSBURGH at NY GIANTS Time...</td>\n",
       "      <td>[Today, 's, NFL, game, PITTSBURGH, NY, GIANTS,...</td>\n",
       "      <td>(Today, 's, NFL, games, PITTSBURGH, at, NY, GI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nets get Carter from Raptors INDIANAPOLIS -- A...</td>\n",
       "      <td>[Nets, get, Carter, Raptors, INDIANAPOLIS, --,...</td>\n",
       "      <td>(Nets, get, Carter, from, Raptors, INDIANAPOLI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        class_idx     class  \\\n",
       "0               3  Business   \n",
       "1               3  Business   \n",
       "2               3  Business   \n",
       "3               3  Business   \n",
       "4               3  Business   \n",
       "...           ...       ...   \n",
       "119995          1     World   \n",
       "119996          2    Sports   \n",
       "119997          2    Sports   \n",
       "119998          2    Sports   \n",
       "119999          2    Sports   \n",
       "\n",
       "                                                     text  \\\n",
       "0       Wall St. Bears Claw Back Into the Black (Reute...   \n",
       "1       Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2       Oil and Economy Cloud Stocks' Outlook (Reuters...   \n",
       "3       Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4       Oil prices soar to all-time record, posing new...   \n",
       "...                                                   ...   \n",
       "119995  Pakistan's Musharraf Says Won't Quit as Army C...   \n",
       "119996  Renteria signing a top-shelf deal Red Sox gene...   \n",
       "119997  Saban not going to Dolphins yet The Miami Dolp...   \n",
       "119998  Today's NFL games PITTSBURGH at NY GIANTS Time...   \n",
       "119999  Nets get Carter from Raptors INDIANAPOLIS -- A...   \n",
       "\n",
       "                                                   tokens  \\\n",
       "0       [Wall, St., Bears, Claw, Back, Black, (, Reute...   \n",
       "1       [Carlyle, Looks, Toward, Commercial, Aerospace...   \n",
       "2       [Oil, Economy, Cloud, Stocks, ', Outlook, (, R...   \n",
       "3       [Iraq, Halts, Oil, Exports, Main, Southern, Pi...   \n",
       "4       [Oil, price, soar, all-time, record, ,, pose, ...   \n",
       "...                                                   ...   \n",
       "119995  [Pakistan, 's, Musharraf, Says, Wo, n't, Quit,...   \n",
       "119996  [Renteria, sign, top-shelf, deal, Red, Sox, ge...   \n",
       "119997  [Saban, go, Dolphins, yet, Miami, Dolphins, pu...   \n",
       "119998  [Today, 's, NFL, game, PITTSBURGH, NY, GIANTS,...   \n",
       "119999  [Nets, get, Carter, Raptors, INDIANAPOLIS, --,...   \n",
       "\n",
       "                                                      doc  \n",
       "0       (Wall, St., Bears, Claw, Back, Into, the, Blac...  \n",
       "1       (Carlyle, Looks, Toward, Commercial, Aerospace...  \n",
       "2       (Oil, and, Economy, Cloud, Stocks, ', Outlook,...  \n",
       "3       (Iraq, Halts, Oil, Exports, from, Main, Southe...  \n",
       "4       (Oil, prices, soar, to, all, -, time, record, ...  \n",
       "...                                                   ...  \n",
       "119995  (Pakistan, 's, Musharraf, Says, Wo, n't, Quit,...  \n",
       "119996  (Renteria, signing, a, top, -, shelf, deal, Re...  \n",
       "119997  (Saban, not, going, to, Dolphins, yet, The, Mi...  \n",
       "119998  (Today, 's, NFL, games, PITTSBURGH, at, NY, GI...  \n",
       "119999  (Nets, get, Carter, from, Raptors, INDIANAPOLI...  \n",
       "\n",
       "[120000 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected output is:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_idx</th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>doc</th>\n",
       "      <th>spacy_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
       "      <td>[Wall, St., Bears, Claw, Back, Black, (, Reute...</td>\n",
       "      <td>(Wall, St., Bears, Claw, Back, Into, the, Blac...</td>\n",
       "      <td>[Wall, St., Bears, Claw, Black, (, Reuters, ),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>[Carlyle, Looks, Toward, Commercial, Aerospace...</td>\n",
       "      <td>(Carlyle, Looks, Toward, Commercial, Aerospace...</td>\n",
       "      <td>[Carlyle, look, Commercial, Aerospace, (, Reut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "      <td>[Oil, Economy, Cloud, Stocks, ', Outlook, (, R...</td>\n",
       "      <td>(Oil, and, Economy, Cloud, Stocks, ', Outlook,...</td>\n",
       "      <td>[oil, Economy, Cloud, Stocks, ', Outlook, (, R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>[Iraq, Halts, Oil, Exports, Main, Southern, Pi...</td>\n",
       "      <td>(Iraq, Halts, Oil, Exports, from, Main, Southe...</td>\n",
       "      <td>[Iraq, Halts, Oil, Exports, Main, Southern, Pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>[Oil, price, soar, all-time, record, ,, pose, ...</td>\n",
       "      <td>(Oil, prices, soar, to, all, -, time, record, ...</td>\n",
       "      <td>[oil, price, soar, -, time, record, ,, pose, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>1</td>\n",
       "      <td>World</td>\n",
       "      <td>Pakistan's Musharraf Says Won't Quit as Army C...</td>\n",
       "      <td>[Pakistan, 's, Musharraf, Says, Wo, n't, Quit,...</td>\n",
       "      <td>(Pakistan, 's, Musharraf, Says, Wo, n't, Quit,...</td>\n",
       "      <td>[Pakistan, Musharraf, say, will, Quit, Army, C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Renteria signing a top-shelf deal Red Sox gene...</td>\n",
       "      <td>[Renteria, sign, top-shelf, deal, Red, Sox, ge...</td>\n",
       "      <td>(Renteria, signing, a, top, -, shelf, deal, Re...</td>\n",
       "      <td>[Renteria, sign, -, shelf, deal, Red, Sox, gen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Saban not going to Dolphins yet The Miami Dolp...</td>\n",
       "      <td>[Saban, go, Dolphins, yet, Miami, Dolphins, pu...</td>\n",
       "      <td>(Saban, not, going, to, Dolphins, yet, The, Mi...</td>\n",
       "      <td>[saban, go, Dolphins, Miami, Dolphins, courtsh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Today's NFL games PITTSBURGH at NY GIANTS Time...</td>\n",
       "      <td>[Today, 's, NFL, game, PITTSBURGH, NY, GIANTS,...</td>\n",
       "      <td>(Today, 's, NFL, games, PITTSBURGH, at, NY, GI...</td>\n",
       "      <td>[today, NFL, game, pittsburgh, NY, giant, time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nets get Carter from Raptors INDIANAPOLIS -- A...</td>\n",
       "      <td>[Nets, get, Carter, Raptors, INDIANAPOLIS, --,...</td>\n",
       "      <td>(Nets, get, Carter, from, Raptors, INDIANAPOLI...</td>\n",
       "      <td>[net, Carter, Raptors, INDIANAPOLIS, --, -, St...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        class_idx     class  \\\n",
       "0               3  Business   \n",
       "1               3  Business   \n",
       "2               3  Business   \n",
       "3               3  Business   \n",
       "4               3  Business   \n",
       "...           ...       ...   \n",
       "119995          1     World   \n",
       "119996          2    Sports   \n",
       "119997          2    Sports   \n",
       "119998          2    Sports   \n",
       "119999          2    Sports   \n",
       "\n",
       "                                                     text  \\\n",
       "0       Wall St. Bears Claw Back Into the Black (Reute...   \n",
       "1       Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2       Oil and Economy Cloud Stocks' Outlook (Reuters...   \n",
       "3       Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4       Oil prices soar to all-time record, posing new...   \n",
       "...                                                   ...   \n",
       "119995  Pakistan's Musharraf Says Won't Quit as Army C...   \n",
       "119996  Renteria signing a top-shelf deal Red Sox gene...   \n",
       "119997  Saban not going to Dolphins yet The Miami Dolp...   \n",
       "119998  Today's NFL games PITTSBURGH at NY GIANTS Time...   \n",
       "119999  Nets get Carter from Raptors INDIANAPOLIS -- A...   \n",
       "\n",
       "                                                   tokens  \\\n",
       "0       [Wall, St., Bears, Claw, Back, Black, (, Reute...   \n",
       "1       [Carlyle, Looks, Toward, Commercial, Aerospace...   \n",
       "2       [Oil, Economy, Cloud, Stocks, ', Outlook, (, R...   \n",
       "3       [Iraq, Halts, Oil, Exports, Main, Southern, Pi...   \n",
       "4       [Oil, price, soar, all-time, record, ,, pose, ...   \n",
       "...                                                   ...   \n",
       "119995  [Pakistan, 's, Musharraf, Says, Wo, n't, Quit,...   \n",
       "119996  [Renteria, sign, top-shelf, deal, Red, Sox, ge...   \n",
       "119997  [Saban, go, Dolphins, yet, Miami, Dolphins, pu...   \n",
       "119998  [Today, 's, NFL, game, PITTSBURGH, NY, GIANTS,...   \n",
       "119999  [Nets, get, Carter, Raptors, INDIANAPOLIS, --,...   \n",
       "\n",
       "                                                      doc  \\\n",
       "0       (Wall, St., Bears, Claw, Back, Into, the, Blac...   \n",
       "1       (Carlyle, Looks, Toward, Commercial, Aerospace...   \n",
       "2       (Oil, and, Economy, Cloud, Stocks, ', Outlook,...   \n",
       "3       (Iraq, Halts, Oil, Exports, from, Main, Southe...   \n",
       "4       (Oil, prices, soar, to, all, -, time, record, ...   \n",
       "...                                                   ...   \n",
       "119995  (Pakistan, 's, Musharraf, Says, Wo, n't, Quit,...   \n",
       "119996  (Renteria, signing, a, top, -, shelf, deal, Re...   \n",
       "119997  (Saban, not, going, to, Dolphins, yet, The, Mi...   \n",
       "119998  (Today, 's, NFL, games, PITTSBURGH, at, NY, GI...   \n",
       "119999  (Nets, get, Carter, from, Raptors, INDIANAPOLI...   \n",
       "\n",
       "                                             spacy_tokens  \n",
       "0       [Wall, St., Bears, Claw, Black, (, Reuters, ),...  \n",
       "1       [Carlyle, look, Commercial, Aerospace, (, Reut...  \n",
       "2       [oil, Economy, Cloud, Stocks, ', Outlook, (, R...  \n",
       "3       [Iraq, Halts, Oil, Exports, Main, Southern, Pi...  \n",
       "4       [oil, price, soar, -, time, record, ,, pose, n...  \n",
       "...                                                   ...  \n",
       "119995  [Pakistan, Musharraf, say, will, Quit, Army, C...  \n",
       "119996  [Renteria, sign, -, shelf, deal, Red, Sox, gen...  \n",
       "119997  [saban, go, Dolphins, Miami, Dolphins, courtsh...  \n",
       "119998  [today, NFL, game, pittsburgh, NY, giant, time...  \n",
       "119999  [net, Carter, Raptors, INDIANAPOLIS, --, -, St...  \n",
       "\n",
       "[120000 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_answer_df(df_train, answer_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the answer version of the `most_used_words` function to again display the top 5 words per class in the dataset. Do you see some differences between the lemmatized tokens obtained from NLTK and spaCy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>spacy_tokens</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24978</th>\n",
       "      <td>Business</td>\n",
       "      <td>say</td>\n",
       "      <td>8926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11350</th>\n",
       "      <td>Business</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>6882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22766</th>\n",
       "      <td>Business</td>\n",
       "      <td>oil</td>\n",
       "      <td>5226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17100</th>\n",
       "      <td>Business</td>\n",
       "      <td>company</td>\n",
       "      <td>5160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23743</th>\n",
       "      <td>Business</td>\n",
       "      <td>price</td>\n",
       "      <td>5051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55134</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>new</td>\n",
       "      <td>5495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37893</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>5058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58221</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>say</td>\n",
       "      <td>5023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48306</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>company</td>\n",
       "      <td>3876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28099</th>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>AP</td>\n",
       "      <td>3692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62256</th>\n",
       "      <td>Sports</td>\n",
       "      <td>AP</td>\n",
       "      <td>6262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94426</th>\n",
       "      <td>Sports</td>\n",
       "      <td>win</td>\n",
       "      <td>6168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85465</th>\n",
       "      <td>Sports</td>\n",
       "      <td>game</td>\n",
       "      <td>4467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92916</th>\n",
       "      <td>Sports</td>\n",
       "      <td>team</td>\n",
       "      <td>3758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91391</th>\n",
       "      <td>Sports</td>\n",
       "      <td>season</td>\n",
       "      <td>3695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121901</th>\n",
       "      <td>World</td>\n",
       "      <td>say</td>\n",
       "      <td>10169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94784</th>\n",
       "      <td>World</td>\n",
       "      <td>AP</td>\n",
       "      <td>5786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101826</th>\n",
       "      <td>World</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>5782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107051</th>\n",
       "      <td>World</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>5414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117855</th>\n",
       "      <td>World</td>\n",
       "      <td>kill</td>\n",
       "      <td>5046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           class spacy_tokens  count\n",
       "24978   Business          say   8926\n",
       "11350   Business      Reuters   6882\n",
       "22766   Business          oil   5226\n",
       "17100   Business      company   5160\n",
       "23743   Business        price   5051\n",
       "55134   Sci/Tech          new   5495\n",
       "37893   Sci/Tech    Microsoft   5058\n",
       "58221   Sci/Tech          say   5023\n",
       "48306   Sci/Tech      company   3876\n",
       "28099   Sci/Tech           AP   3692\n",
       "62256     Sports           AP   6262\n",
       "94426     Sports          win   6168\n",
       "85465     Sports         game   4467\n",
       "92916     Sports         team   3758\n",
       "91391     Sports       season   3695\n",
       "121901     World          say  10169\n",
       "94784      World           AP   5786\n",
       "101826     World         Iraq   5782\n",
       "107051     World      Reuters   5414\n",
       "117855     World         kill   5046"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(answer_most_used_words(answer_df, 'spacy_tokens'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"t5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Unsupervised Learning - Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling is a technique used in NLP that aims to identify the underlying topics or themes in a collection of texts. One way to perform topic modelling is using the probabilistic model Latent Dirichlet Allocation (LDA).\n",
    "\n",
    "LDA assumes that each document in a collection is a mixture of different topics, and each topic is a probability distribution over a set of words. The model then infers the underlying topic distribution for each document in the collection and the word distribution for each topic. LDA is trained using an iterative algorithm that maximizes the likelihood of observing the given documents.\n",
    "\n",
    "To use LDA, we need to represent the documents as a bag of words, where the order of the words is ignored and only the frequency of each word in the document is considered. This bag-of-words representation allows us to represent each document as a vector of word frequencies, which can be used as input to the LDA algorithm. Computing LDA might take a moment on our dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n"
     ]
    }
   ],
   "source": [
    "# Define the number of topics to model with LDA.\n",
    "num_topics = 4\n",
    "\n",
    "# Convert preprocessed text to bag-of-words representation using CountVectorizer.\n",
    "vectorizer = CountVectorizer(max_features=50000)\n",
    "\n",
    "# fit_transform requires either a string as input or multiple extra arguments and functions, so turn tokens into string.\n",
    "X = vectorizer.fit_transform(answer_df['spacy_tokens'].apply(lambda x: ' '.join(x)).values)\n",
    "\n",
    "# Fit LDA to the feature matrix. Verbose so we know what iteration we're on.\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, max_iter=10, random_state=42, verbose=True)\n",
    "lda.fit(X)\n",
    "\n",
    "# Extract the topic proportions for each document.\n",
    "doc_topic_proportions = lda.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function, we can look at the most important words per topic. Do you see any similarities with the most occuring words per class after stopword removal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['39', 'win', 'ap', 'game', 'team'],\n",
       " 1: ['say', '39', 'ap', 'iraq', 'president'],\n",
       " 2: ['reuters', 'oil', 'stock', 'price', 'say'],\n",
       " 3: ['39', 'new', 'company', 'say', 'microsoft']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def n_top_wordlist(model, features, ntopwords=5):\n",
    "    \"\"\"\n",
    "    Add a column with a list of lemmatized tokens, without stopwords.\n",
    "    \"\"\"\n",
    "    output = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        output[topic_idx] = [features[i] for i in topic.argsort()[:-ntopwords - 1:-1]]\n",
    "    return output\n",
    "\n",
    "# Get the words from the CountVectorizer.\n",
    "tf_feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "display(n_top_wordlist(lda, tf_feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"t5-1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusted Mutual Information (AMI) and Adjusted Rand Index (ARI) are two metrics used to evaluate the performance of clustering algorithms.\n",
    "\n",
    "AMI is a measure that takes into account the possibility of two random clusters appearing to be similar. It is calculated as the difference between the Mutual Information (MI) of two clusterings and the expected MI, divided by the average entropy of the two clusterings minus the expected MI. AMI ranges between 0 and 1, where 0 indicates no agreement between the two clusterings and 1 indicates identical clusterings.\n",
    "\n",
    "The Rand Index (RI) is a measure that counts the number of pairs of samples that are assigned to the same or different clusters in both the predicted and true clusterings. The raw RI score is then adjusted for chance into the ARI score using a scheme similar to that of AMI. For ARI a score of 0 indicates random labeling and 1 indicates perfect agreement. The ARI is bounded below by -0.5 for very large differences in labeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"a5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment for Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your task (which is your assignment) is to write a function to do the following:**\n",
    "- The `doc_topic_proportions` contains the proportions of how much that document belongs to every topic. For every document, get the topic in which it has the largest proportion. Afterwards, look at the AMI and ARI scores. Can you improve the scores by modeling more topics, using a different set of tokens or by using more epochs?\n",
    "    - Hint: use the `numpy.argmax` function.\n",
    "    \n",
    "    Our goal is to get an array which looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 2 ... 0 0 0] (120000,)\n"
     ]
    }
   ],
   "source": [
    "answer_topic_most = answer_largest_proportion(doc_topic_proportions)\n",
    "print(answer_topic_most, answer_topic_most.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def largest_proportion(arr):\n",
    "    \"\"\"\n",
    "    For every row, get the column number where it has the largest value.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : numpy.array\n",
    "        The array with the amount of topics as the amount of columns\n",
    "        and the amount of documents as the number of rows.\n",
    "        Every row should sum up to 1.\n",
    "         \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.array\n",
    "        The 1-dimensional array containing the label of the topic\n",
    "        the document has the largest proportion in.\n",
    "    \"\"\"\n",
    "    ###################################\n",
    "    # Fill in your answer here\n",
    "    return None\n",
    "    ###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below tests if the function matches the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test case 1 failed.\n",
      "Your output is:\n",
      "None\n",
      "Expected output is:\n",
      "[2 3 2 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "topic_most = largest_proportion(doc_topic_proportions)\n",
    "\n",
    "check_answer_np(topic_most, answer_topic_most)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted mutual information score: 0.46\n",
      "Adjusted rand score: 0.45\n"
     ]
    }
   ],
   "source": [
    "ami_score = adjusted_mutual_info_score(df_train['class'], answer_topic_most)\n",
    "ari_score = adjusted_rand_score(df_train['class'], answer_topic_most)\n",
    "\n",
    "print(f\"Adjusted mutual information score: {ami_score:.2f}\")\n",
    "print(f\"Adjusted rand score: {ari_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some topics get (way) more documents assigned to them than others? Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 34525, 1: 33808, 2: 19254, 3: 32413}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(answer_topic_most, return_counts=True)\n",
    "\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"t6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings represent words as vectors in a high-dimensional space. The key idea behind word embeddings is that words with similar meanings tend to appear in similar contexts, and therefore their vector representations should be close together in this high-dimensional space. Word embeddings have been widely used in various NLP tasks such as sentiment analysis, machine translation, and information retrieval.\n",
    "\n",
    "There are several techniques to generate word embeddings, but one of the most popular methods is the Word2Vec algorithm, which is based on a neural network architecture. Word2Vec learns embeddings by predicting the probability of a word given its context (continuous bag of words or skip-gram model). The output of the network is a set of word vectors that can be used as embeddings.\n",
    "\n",
    "We can train a Word2Vec model ourselves, but keep in mind that later on it's not nice if we don't have embeddings for certain words in the test set. So let's first apply the familiar preprocessing steps to the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908f11ed53854281aea32d1dd21aa5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e235db8c8ea4d4b8cf0fe3ae023e0f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831c00274d3d47b29f5eb2d2c5f75e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_idx</th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>doc</th>\n",
       "      <th>spacy_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Fears for T N pension after talks Unions repre...</td>\n",
       "      <td>[Fears, N, pension, talk, Unions, represent, w...</td>\n",
       "      <td>(Fears, for, T, N, pension, after, talks, Unio...</td>\n",
       "      <td>[fear, T, N, pension, talk, Unions, represent,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>The Race is On: Second Private Team Sets Launc...</td>\n",
       "      <td>[Race, :, Second, Private, Team, Sets, Launch,...</td>\n",
       "      <td>(The, Race, is, On, :, Second, Private, Team, ...</td>\n",
       "      <td>[Race, :, second, private, team, set, Launch, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>Ky. Company Wins Grant to Study Peptides (AP) ...</td>\n",
       "      <td>[Ky., Company, Wins, Grant, Study, Peptides, (...</td>\n",
       "      <td>(Ky., Company, Wins, Grant, to, Study, Peptide...</td>\n",
       "      <td>[Ky., Company, win, Grant, study, Peptides, (,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>Prediction Unit Helps Forecast Wildfires (AP) ...</td>\n",
       "      <td>[Prediction, Unit, Helps, Forecast, Wildfires,...</td>\n",
       "      <td>(Prediction, Unit, Helps, Forecast, Wildfires,...</td>\n",
       "      <td>[prediction, Unit, help, Forecast, Wildfires, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>Calif. Aims to Limit Farm-Related Smog (AP) AP...</td>\n",
       "      <td>[Calif, ., Aims, Limit, Farm-Related, Smog, (,...</td>\n",
       "      <td>(Calif., Aims, to, Limit, Farm, -, Related, Sm...</td>\n",
       "      <td>[Calif., aim, limit, Farm, -, Related, Smog, (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7595</th>\n",
       "      <td>1</td>\n",
       "      <td>World</td>\n",
       "      <td>Around the world Ukrainian presidential candid...</td>\n",
       "      <td>[Around, world, Ukrainian, presidential, candi...</td>\n",
       "      <td>(Around, the, world, Ukrainian, presidential, ...</td>\n",
       "      <td>[world, ukrainian, presidential, candidate, Vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7596</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Void is filled with Clement With the supply of...</td>\n",
       "      <td>[Void, fill, Clement, supply, attractive, pitc...</td>\n",
       "      <td>(Void, is, filled, with, Clement, With, the, s...</td>\n",
       "      <td>[Void, fill, Clement, supply, attractive, pitc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7597</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Martinez leaves bitter Like Roger Clemens did ...</td>\n",
       "      <td>[Martinez, leave, bitter, Like, Roger, Clemens...</td>\n",
       "      <td>(Martinez, leaves, bitter, Like, Roger, Clemen...</td>\n",
       "      <td>[Martinez, leave, bitter, like, Roger, Clemens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7598</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>5 of arthritis patients in Singapore take Bext...</td>\n",
       "      <td>[5, arthritis, patient, Singapore, take, Bextr...</td>\n",
       "      <td>(5, of, arthritis, patients, in, Singapore, ta...</td>\n",
       "      <td>[5, arthritis, patient, Singapore, Bextra, Cel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7599</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>EBay gets into rentals EBay plans to buy the a...</td>\n",
       "      <td>[EBay, get, rental, EBay, plan, buy, apartment...</td>\n",
       "      <td>(EBay, gets, into, rentals, EBay, plans, to, b...</td>\n",
       "      <td>[EBay, get, rental, EBay, plan, buy, apartment...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7600 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      class_idx     class                                               text  \\\n",
       "0             3  Business  Fears for T N pension after talks Unions repre...   \n",
       "1             4  Sci/Tech  The Race is On: Second Private Team Sets Launc...   \n",
       "2             4  Sci/Tech  Ky. Company Wins Grant to Study Peptides (AP) ...   \n",
       "3             4  Sci/Tech  Prediction Unit Helps Forecast Wildfires (AP) ...   \n",
       "4             4  Sci/Tech  Calif. Aims to Limit Farm-Related Smog (AP) AP...   \n",
       "...         ...       ...                                                ...   \n",
       "7595          1     World  Around the world Ukrainian presidential candid...   \n",
       "7596          2    Sports  Void is filled with Clement With the supply of...   \n",
       "7597          2    Sports  Martinez leaves bitter Like Roger Clemens did ...   \n",
       "7598          3  Business  5 of arthritis patients in Singapore take Bext...   \n",
       "7599          3  Business  EBay gets into rentals EBay plans to buy the a...   \n",
       "\n",
       "                                                 tokens  \\\n",
       "0     [Fears, N, pension, talk, Unions, represent, w...   \n",
       "1     [Race, :, Second, Private, Team, Sets, Launch,...   \n",
       "2     [Ky., Company, Wins, Grant, Study, Peptides, (...   \n",
       "3     [Prediction, Unit, Helps, Forecast, Wildfires,...   \n",
       "4     [Calif, ., Aims, Limit, Farm-Related, Smog, (,...   \n",
       "...                                                 ...   \n",
       "7595  [Around, world, Ukrainian, presidential, candi...   \n",
       "7596  [Void, fill, Clement, supply, attractive, pitc...   \n",
       "7597  [Martinez, leave, bitter, Like, Roger, Clemens...   \n",
       "7598  [5, arthritis, patient, Singapore, take, Bextr...   \n",
       "7599  [EBay, get, rental, EBay, plan, buy, apartment...   \n",
       "\n",
       "                                                    doc  \\\n",
       "0     (Fears, for, T, N, pension, after, talks, Unio...   \n",
       "1     (The, Race, is, On, :, Second, Private, Team, ...   \n",
       "2     (Ky., Company, Wins, Grant, to, Study, Peptide...   \n",
       "3     (Prediction, Unit, Helps, Forecast, Wildfires,...   \n",
       "4     (Calif., Aims, to, Limit, Farm, -, Related, Sm...   \n",
       "...                                                 ...   \n",
       "7595  (Around, the, world, Ukrainian, presidential, ...   \n",
       "7596  (Void, is, filled, with, Clement, With, the, s...   \n",
       "7597  (Martinez, leaves, bitter, Like, Roger, Clemen...   \n",
       "7598  (5, of, arthritis, patients, in, Singapore, ta...   \n",
       "7599  (EBay, gets, into, rentals, EBay, plans, to, b...   \n",
       "\n",
       "                                           spacy_tokens  \n",
       "0     [fear, T, N, pension, talk, Unions, represent,...  \n",
       "1     [Race, :, second, private, team, set, Launch, ...  \n",
       "2     [Ky., Company, win, Grant, study, Peptides, (,...  \n",
       "3     [prediction, Unit, help, Forecast, Wildfires, ...  \n",
       "4     [Calif., aim, limit, Farm, -, Related, Smog, (...  \n",
       "...                                                 ...  \n",
       "7595  [world, ukrainian, presidential, candidate, Vi...  \n",
       "7596  [Void, fill, Clement, supply, attractive, pitc...  \n",
       "7597  [Martinez, leave, bitter, like, Roger, Clemens...  \n",
       "7598  [5, arthritis, patient, Singapore, Bextra, Cel...  \n",
       "7599  [EBay, get, rental, EBay, plan, buy, apartment...  \n",
       "\n",
       "[7600 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reformat df_test.\n",
    "df_test = reformat_data(df_test)\n",
    "\n",
    "# NLTK preprocessing.\n",
    "df_test = answer_remove_stopwords(answer_tokenize_and_lemmatize(df_test))\n",
    "\n",
    "# spaCy preprocessing.\n",
    "df_test = answer_spacy_tokens(add_spacy(df_test))\n",
    "\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the complete model, we combine the `tokens` column into one series and call the `Word2Vec` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all tokens into one series.\n",
    "tokens_both = pd.concat([df_train['tokens'], df_test['tokens']])\n",
    "\n",
    "# Train a Word2Vec model on the NLTK tokens.\n",
    "w2v_model = Word2Vec(tokens_both.values, vector_size=96, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the embeddings, we can use the `Word2Vec.wv[word]` syntax. To get multiple vectors nicely next to eachother in a 2D matrix, we can call `numpy.vstack`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.28653669e+00 -1.23982191e+00 -3.78573418e-01  1.24864781e+00\n",
      "  -1.02406311e+00  2.70144081e+00  4.39260215e-01 -8.99205863e-01\n",
      "   8.06533873e-01 -9.62060988e-02 -1.11389792e+00  1.88659757e-01\n",
      "  -3.85470480e-01 -6.15484118e-01  2.10264111e+00  1.58280671e+00\n",
      "   1.16154599e+00 -1.80119365e-01  2.15870523e+00  1.51180661e+00\n",
      "   2.43140250e-01 -2.13228241e-01 -2.91460657e+00 -1.04477501e+00\n",
      "   6.76880479e-01 -1.14109051e+00  2.02643454e-01 -3.91020998e-02\n",
      "   7.75238156e-01 -5.73292911e-01  9.78131235e-01 -7.68854022e-01\n",
      "   1.32406697e-01  5.30529976e-01 -2.18397236e+00  4.62752014e-01\n",
      "   6.03099279e-02 -1.72177162e-02 -2.52509642e+00  8.45575213e-01\n",
      "   9.80891407e-01  3.52037162e-01 -1.52029052e-01  7.10558474e-01\n",
      "   3.22469682e-01 -1.58625990e-02 -3.43625307e-01  3.44772935e-01\n",
      "  -8.66874456e-01  8.47807825e-01 -3.53041142e-01  8.56600702e-02\n",
      "   7.54837573e-01  1.78848755e+00  1.85996187e+00  4.74622488e-01\n",
      "  -6.65434659e-01  1.83964300e+00 -5.08127749e-01  1.93614209e+00\n",
      "   1.28386939e+00  4.86888409e-01 -7.73515701e-01  8.66592467e-01\n",
      "   1.29312742e+00 -1.38573170e+00 -4.15464312e-01  6.39338885e-03\n",
      "  -1.10148795e-01 -1.39587927e+00 -1.04105914e+00 -4.55558270e-01\n",
      "   1.41976452e+00 -2.11662221e+00 -2.22652841e+00 -6.92496225e-02\n",
      "  -6.69126272e-01 -1.22268653e+00 -1.34468526e-01 -7.57689893e-01\n",
      "   1.12099266e+00 -1.00496733e+00  1.55857608e-01 -1.44416487e+00\n",
      "   6.08251691e-01  2.28119940e-01 -1.41050816e+00 -1.05514161e-01\n",
      "   1.04892170e+00  8.91954780e-01  1.94006371e+00  3.86030614e-01\n",
      "   4.43993181e-01 -7.52807200e-01  7.47733295e-01  1.28528917e+00]\n",
      " [-4.33241189e-01  5.13795726e-02  1.90634262e-02 -2.39672381e-02\n",
      "  -4.94390279e-02 -1.74405947e-01  2.30662376e-01 -3.78533185e-01\n",
      "  -2.93898821e-01  5.35730310e-02  1.93949714e-02  7.49628693e-02\n",
      "   1.07567020e-01 -6.11311734e-01 -4.27345246e-01  3.99061799e-01\n",
      "  -5.03581405e-01  9.89489853e-02 -2.83894241e-01  2.42355257e-01\n",
      "   5.07354498e-01  2.40523860e-01  6.89382628e-02 -5.83623163e-02\n",
      "  -6.99409321e-02 -6.39237285e-01 -5.80131054e-01 -4.81335700e-01\n",
      "  -6.53606781e-04  1.63093824e-02 -1.30442664e-01  1.99743792e-01\n",
      "  -1.03107043e-01  1.18938945e-01  2.90249363e-02  5.46019599e-02\n",
      "   3.57755989e-01 -5.97061515e-01  1.02643743e-01  4.82714474e-01\n",
      "   2.79673308e-01 -2.51711607e-01  5.98427892e-01  2.42832303e-01\n",
      "   9.57599282e-02 -6.71500862e-02 -1.71847299e-01  2.23192275e-01\n",
      "  -4.57851350e-01  6.67518824e-02  6.50244430e-02  7.97375143e-02\n",
      "  -4.95795831e-02  2.94979755e-02  3.32861543e-01 -1.39063269e-01\n",
      "   1.25975713e-01  4.06735450e-01 -2.29404300e-01  2.04156443e-01\n",
      "   5.00620484e-01 -2.20501795e-01 -7.41276294e-02  1.81492507e-01\n",
      "   1.99564517e-01 -5.75285137e-01  1.62974790e-01  3.74771893e-01\n",
      "   4.31558713e-02 -2.68922657e-01  1.53414264e-01 -9.92786139e-02\n",
      "  -1.64403379e-01  1.58746779e-01 -3.65101159e-01 -4.53409016e-01\n",
      "   3.49858046e-01  3.26217979e-01 -5.00941217e-01  9.63574648e-02\n",
      "   1.37866795e-01  3.08415685e-02 -2.19391063e-01  3.74443620e-01\n",
      "  -9.08634886e-02  3.40778708e-01 -1.97385237e-01 -6.21022768e-02\n",
      "   1.26044050e-01  1.83011338e-01  4.30193692e-01 -3.46136093e-02\n",
      "   4.88837600e-01 -1.13159791e-01  4.44456071e-01  3.17715019e-01]\n",
      " [-7.14830399e-01 -2.44312063e-01 -3.30759287e-01  5.79907969e-02\n",
      "  -4.01968241e-01  5.27886927e-01  5.33978701e-01 -4.84708518e-01\n",
      "  -2.53215760e-01  3.57973248e-01  6.18736625e-01 -3.35471332e-01\n",
      "  -2.84158230e-01 -8.30504596e-01 -6.04465544e-01  7.01055825e-01\n",
      "  -3.83586943e-01 -7.14636315e-03 -3.11047345e-01  1.17181443e-01\n",
      "   5.96607625e-01  8.74696255e-01 -7.72337839e-02 -4.12481189e-01\n",
      "  -7.31183738e-02 -7.66396344e-01 -4.70926434e-01 -3.74659270e-01\n",
      "  -2.04756305e-01 -1.09709345e-01 -1.32373795e-01 -4.01511282e-01\n",
      "  -1.64800789e-02  1.02150947e-01 -4.94151982e-03  7.93356836e-01\n",
      "   6.21815026e-01 -8.94524455e-01 -9.51696783e-02  5.69100738e-01\n",
      "   6.22797668e-01 -5.46574257e-02  6.26099169e-01  3.13354313e-01\n",
      "   1.03145495e-01 -4.14715230e-01 -3.15910071e-01  2.62431443e-01\n",
      "  -6.23409569e-01  3.78114879e-01 -2.35012740e-01  8.94739211e-01\n",
      "   3.66743445e-01 -2.07462713e-01  6.39247894e-01 -4.81571287e-01\n",
      "  -6.01958632e-02  1.24626791e+00 -8.94501567e-01  3.10852617e-01\n",
      "   3.67685139e-01 -1.25022650e-01 -9.21079367e-02  2.54476517e-01\n",
      "   6.18484497e-01 -4.61069435e-01 -2.22754598e-01  5.45891225e-01\n",
      "   2.34197661e-01  3.19173723e-03  2.61288024e-02 -5.31158626e-01\n",
      "  -4.93690610e-01  1.33706182e-01 -6.11238122e-01 -3.53407800e-01\n",
      "   5.96068144e-01  1.52272820e-01 -2.13226438e-01 -2.94075370e-01\n",
      "  -1.26790076e-01 -2.81025976e-01 -1.01347439e-01  8.91439497e-01\n",
      "  -3.08927000e-02  5.64610302e-01 -6.82735562e-01 -1.09319746e-01\n",
      "   4.39243138e-01 -2.08817065e-01  5.58632076e-01 -6.07702360e-02\n",
      "   8.02230954e-01 -1.67870566e-01  7.26805270e-01  4.67041641e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(np.vstack([w2v_model.wv[word] for word in [\"rain\", \"cat\", \"dog\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spaCy model we used has a `Tok2Vec` algorithm in its pipeline, so we can directly access the the 2D matrix of all word vectors on a document with the `Doc.tensor` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.8194096  -0.32325384  0.629434   ... -0.4773795  -0.75188184\n",
      "   0.0357812 ]\n",
      " [ 0.81603235 -1.2405076   0.9558864  ...  0.52738035 -1.0743449\n",
      "  -0.30024663]\n",
      " [ 1.1372288  -1.0574455  -0.20238371 ...  0.38068694 -0.03450352\n",
      "   0.540362  ]\n",
      " ...\n",
      " [ 1.2355547  -0.6400209  -0.59921527 ... -0.12730186 -0.3426052\n",
      "  -1.1101209 ]\n",
      " [ 0.19090153 -0.6523549  -0.4373727  ...  0.8468437   0.49040866\n",
      "  -0.14062503]\n",
      " [-0.5942377  -0.93374133  0.54625034 ... -0.05576921 -0.9447482\n",
      "  -1.1440233 ]]\n"
     ]
    }
   ],
   "source": [
    "print(doc.tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"a6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment for Task 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your task (which is your assignment) is to write a function to do the following:**\n",
    "\n",
    "- First, sample 10% from both datasets, we're not going to be using all the data for the neural network.\n",
    "    - Hint: use the `pandas.DataFrame.sample` function to sample a fraction of the data. Specify a `random_state` value to always get the same rows from the dataframe.\n",
    "- Add a `tensor` column to both the test and train dataframes, the column should hold one array per row, containing all the word embedding vectors as columns. You can chose whether to use vectors from our new model or the ones from spaCy.\n",
    "- Determine the largest amount of columns in the `tensor` column, between both datasets.\n",
    "    - Hint: use the `numpy.ndarray.shape` attribute to see the dimensions of an array. \n",
    "    - Hint: use the `pd.Series.max` function to determine the largest item in a series.\n",
    "- Pad all arrays in the `tensor` column to be equal in size to the biggest tensor, by adding columns of zeroes in the end. This way all inputs for a neural network have the same size.\n",
    "    - Hint: use the `numpy.pad` function to pad an aray.\n",
    "    \n",
    "    After the function, our `df_train` could look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_idx</th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>doc</th>\n",
       "      <th>spacy_tokens</th>\n",
       "      <th>tensor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71787</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>BBC set for major shake-up, claims newspaper L...</td>\n",
       "      <td>[BBC, set, major, shake-up, ,, claim, newspape...</td>\n",
       "      <td>(BBC, set, for, major, shake, -, up, ,, claims...</td>\n",
       "      <td>[BBC, set, major, shake, -, ,, claim, newspape...</td>\n",
       "      <td>[[-1.2082729, -0.3078657, -0.024013683, 0.8767...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67218</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Marsh averts cash crunch Embattled insurance b...</td>\n",
       "      <td>[Marsh, avert, cash, crunch, Embattled, insura...</td>\n",
       "      <td>(Marsh, averts, cash, crunch, Embattled, insur...</td>\n",
       "      <td>[Marsh, avert, cash, crunch, embattle, insuran...</td>\n",
       "      <td>[[-0.5161524, -1.0432737, 0.41055828, 0.692814...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54066</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Jeter, Yankees Look to Take Control (AP) AP - ...</td>\n",
       "      <td>[Jeter, ,, Yankees, Look, Take, Control, (, AP...</td>\n",
       "      <td>(Jeter, ,, Yankees, Look, to, Take, Control, (...</td>\n",
       "      <td>[Jeter, ,, Yankees, look, Control, (, AP, ), A...</td>\n",
       "      <td>[[-1.8953018, 0.86324644, 0.094712, 0.8372249,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7168</th>\n",
       "      <td>4</td>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>Flying the Sun to Safety When the Genesis caps...</td>\n",
       "      <td>[Flying, Sun, Safety, Genesis, capsule, come, ...</td>\n",
       "      <td>(Flying, the, Sun, to, Safety, When, the, Gene...</td>\n",
       "      <td>[fly, Sun, Safety, Genesis, capsule, come, Ear...</td>\n",
       "      <td>[[-1.7130511, 0.37374157, 0.010348439, 0.47499...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29618</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Stocks Seen Flat as Nortel and Oil Weigh  NEW ...</td>\n",
       "      <td>[Stocks, Seen, Flat, Nortel, Oil, Weigh, NEW, ...</td>\n",
       "      <td>(Stocks, Seen, Flat, as, Nortel, and, Oil, Wei...</td>\n",
       "      <td>[stock, see, flat, Nortel, Oil, Weigh,  , NEW,...</td>\n",
       "      <td>[[-1.336605, 1.0966985, -1.0845684, 1.1307448,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27162</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Oakland Athletics Team Report - September 14 (...</td>\n",
       "      <td>[Oakland, Athletics, Team, Report, -, Septembe...</td>\n",
       "      <td>(Oakland, Athletics, Team, Report, -, Septembe...</td>\n",
       "      <td>[Oakland, Athletics, Team, Report, -, Septembe...</td>\n",
       "      <td>[[0.047526084, -0.40058374, -0.34343717, 0.013...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82268</th>\n",
       "      <td>4</td>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>Telcos' convergence strategies diverge CANNES ...</td>\n",
       "      <td>[Telcos, ', convergence, strategy, diverge, CA...</td>\n",
       "      <td>(Telcos, ', convergence, strategies, diverge, ...</td>\n",
       "      <td>[Telcos, ', convergence, strategy, diverge, CA...</td>\n",
       "      <td>[[-0.87280345, -0.7925482, 0.6395072, 2.060443...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7765</th>\n",
       "      <td>4</td>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>Motive aims to head off system glitches Three ...</td>\n",
       "      <td>[Motive, aim, head, system, glitch, Three, new...</td>\n",
       "      <td>(Motive, aims, to, head, off, system, glitches...</td>\n",
       "      <td>[motive, aim, head, system, glitch, new, softw...</td>\n",
       "      <td>[[-1.2538671, -0.69565564, 0.45192608, 0.78003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25871</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>Campbell #39;s 4th-Qtr Net Drops 20 on Higher ...</td>\n",
       "      <td>[Campbell, #, 39, ;, 4th-Qtr, Net, Drops, 20, ...</td>\n",
       "      <td>(Campbell, #, 39;s, 4th, -, Qtr, Net, Drops, 2...</td>\n",
       "      <td>[campbell, #, 39;s, 4th, -, qtr, Net, Drops, 2...</td>\n",
       "      <td>[[-0.72987264, -0.29359108, -0.6149194, 0.4594...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57234</th>\n",
       "      <td>4</td>\n",
       "      <td>Sci/Tech</td>\n",
       "      <td>MySQL to make use of Microsoft code The code, ...</td>\n",
       "      <td>[MySQL, make, use, Microsoft, code, code, ,, o...</td>\n",
       "      <td>(MySQL, to, make, use, of, Microsoft, code, Th...</td>\n",
       "      <td>[mysql, use, Microsoft, code, code, ,, open, -...</td>\n",
       "      <td>[[-0.9036573, 0.69798005, 0.1628182, -0.535712...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       class_idx     class                                               text  \\\n",
       "71787          3  Business  BBC set for major shake-up, claims newspaper L...   \n",
       "67218          3  Business  Marsh averts cash crunch Embattled insurance b...   \n",
       "54066          2    Sports  Jeter, Yankees Look to Take Control (AP) AP - ...   \n",
       "7168           4  Sci/Tech  Flying the Sun to Safety When the Genesis caps...   \n",
       "29618          3  Business  Stocks Seen Flat as Nortel and Oil Weigh  NEW ...   \n",
       "...          ...       ...                                                ...   \n",
       "27162          2    Sports  Oakland Athletics Team Report - September 14 (...   \n",
       "82268          4  Sci/Tech  Telcos' convergence strategies diverge CANNES ...   \n",
       "7765           4  Sci/Tech  Motive aims to head off system glitches Three ...   \n",
       "25871          3  Business  Campbell #39;s 4th-Qtr Net Drops 20 on Higher ...   \n",
       "57234          4  Sci/Tech  MySQL to make use of Microsoft code The code, ...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "71787  [BBC, set, major, shake-up, ,, claim, newspape...   \n",
       "67218  [Marsh, avert, cash, crunch, Embattled, insura...   \n",
       "54066  [Jeter, ,, Yankees, Look, Take, Control, (, AP...   \n",
       "7168   [Flying, Sun, Safety, Genesis, capsule, come, ...   \n",
       "29618  [Stocks, Seen, Flat, Nortel, Oil, Weigh, NEW, ...   \n",
       "...                                                  ...   \n",
       "27162  [Oakland, Athletics, Team, Report, -, Septembe...   \n",
       "82268  [Telcos, ', convergence, strategy, diverge, CA...   \n",
       "7765   [Motive, aim, head, system, glitch, Three, new...   \n",
       "25871  [Campbell, #, 39, ;, 4th-Qtr, Net, Drops, 20, ...   \n",
       "57234  [MySQL, make, use, Microsoft, code, code, ,, o...   \n",
       "\n",
       "                                                     doc  \\\n",
       "71787  (BBC, set, for, major, shake, -, up, ,, claims...   \n",
       "67218  (Marsh, averts, cash, crunch, Embattled, insur...   \n",
       "54066  (Jeter, ,, Yankees, Look, to, Take, Control, (...   \n",
       "7168   (Flying, the, Sun, to, Safety, When, the, Gene...   \n",
       "29618  (Stocks, Seen, Flat, as, Nortel, and, Oil, Wei...   \n",
       "...                                                  ...   \n",
       "27162  (Oakland, Athletics, Team, Report, -, Septembe...   \n",
       "82268  (Telcos, ', convergence, strategies, diverge, ...   \n",
       "7765   (Motive, aims, to, head, off, system, glitches...   \n",
       "25871  (Campbell, #, 39;s, 4th, -, Qtr, Net, Drops, 2...   \n",
       "57234  (MySQL, to, make, use, of, Microsoft, code, Th...   \n",
       "\n",
       "                                            spacy_tokens  \\\n",
       "71787  [BBC, set, major, shake, -, ,, claim, newspape...   \n",
       "67218  [Marsh, avert, cash, crunch, embattle, insuran...   \n",
       "54066  [Jeter, ,, Yankees, look, Control, (, AP, ), A...   \n",
       "7168   [fly, Sun, Safety, Genesis, capsule, come, Ear...   \n",
       "29618  [stock, see, flat, Nortel, Oil, Weigh,  , NEW,...   \n",
       "...                                                  ...   \n",
       "27162  [Oakland, Athletics, Team, Report, -, Septembe...   \n",
       "82268  [Telcos, ', convergence, strategy, diverge, CA...   \n",
       "7765   [motive, aim, head, system, glitch, new, softw...   \n",
       "25871  [campbell, #, 39;s, 4th, -, qtr, Net, Drops, 2...   \n",
       "57234  [mysql, use, Microsoft, code, code, ,, open, -...   \n",
       "\n",
       "                                                  tensor  \n",
       "71787  [[-1.2082729, -0.3078657, -0.024013683, 0.8767...  \n",
       "67218  [[-0.5161524, -1.0432737, 0.41055828, 0.692814...  \n",
       "54066  [[-1.8953018, 0.86324644, 0.094712, 0.8372249,...  \n",
       "7168   [[-1.7130511, 0.37374157, 0.010348439, 0.47499...  \n",
       "29618  [[-1.336605, 1.0966985, -1.0845684, 1.1307448,...  \n",
       "...                                                  ...  \n",
       "27162  [[0.047526084, -0.40058374, -0.34343717, 0.013...  \n",
       "82268  [[-0.87280345, -0.7925482, 0.6395072, 2.060443...  \n",
       "7765   [[-1.2538671, -0.69565564, 0.45192608, 0.78003...  \n",
       "25871  [[-0.72987264, -0.29359108, -0.6149194, 0.4594...  \n",
       "57234  [[-0.9036573, 0.69798005, 0.1628182, -0.535712...  \n",
       "\n",
       "[12000 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer_df_train, answer_df_test = answer_add_padded_tensors(answer_df, df_test)\n",
    "display(answer_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padded_tensors(df1, df2):\n",
    "    \"\"\"\n",
    "    First, sample 10% of both datasets and only use that.\n",
    "    Then, add a tensor column to the dataframes, with every tensor having the same dimensions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_train : pandas.DataFrame\n",
    "        The first dataframe containing at least the tokens or doc columns.\n",
    "    df_test : pandas.DataFrame\n",
    "        The second dataframe containing at least the tokens or doc columns.\n",
    "         \n",
    "    Returns\n",
    "    -------\n",
    "    tuple[pandas.DataFrame]\n",
    "        The sampled dataframes with the added tensor column.\n",
    "    \"\"\"\n",
    "    ###################################\n",
    "    # Fill in your answer here\n",
    "    return None\n",
    "    ###################################\n",
    "\n",
    "df_train, df_test = df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below tests if the function matches the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test case 1 failed. Not all tensor sizes are equal.\n",
      "Test case 2 failed. The test dataframe does not have the correct size.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    assert df_train['tensor'].apply(lambda x: x.shape).unique() == df_test['tensor'].apply(lambda x: x.shape).unique()\n",
    "    print(\"Test case 1 passed.\")\n",
    "except:\n",
    "    print(\"Test case 1 failed. Not all tensor sizes are equal.\")\n",
    "    \n",
    "try:\n",
    "    assert df_test.shape[0] == 760\n",
    "    print(\"Test case 1 passed.\")\n",
    "except:\n",
    "    print(\"Test case 2 failed. The test dataframe does not have the correct size.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"t7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Supervised Learning - Topic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic classification is a task in NLP that involves automatically assigning a given text document to one or more predefined categories or topics. This task is essential for various applications, such as document organization, search engines, sentiment analysis, and more.\n",
    "\n",
    "In recent years, deep learning models have shown remarkable performance in various NLP tasks, including topic classification. We will explore a neural network-based approach for topic classification using the PyTorch framework. The PyTorch library provides an efficient way to build and train neural networks with a high degree of flexibility and ease of use.\n",
    "\n",
    "Our neural network will take the embedding representation of the document as input and predict the corresponding topic using a softmax output layer. We will evaluate the performance of our model using various metrics such as accuracy, precision, recall, and F1-score.\n",
    "\n",
    "The following code demonstrates how to implement a neural network for topic classification in PyTorch. First let's do some more preperations for our inputs, turning them into PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dataframes from previous assignment. These use the spaCy tensors.\n",
    "df_train = answer_df_train\n",
    "df_test = answer_df_test\n",
    "\n",
    "# Transform inputs into PyTorch tensors.\n",
    "input_train = torch.from_numpy(np.stack(df_train['tensor']))\n",
    "input_test = torch.from_numpy(np.stack(df_test['tensor']))\n",
    "\n",
    "# Get the labels, move to 0-indexed instead of 1-indexed.\n",
    "train_labels = torch.from_numpy(df_train['class_idx'].values) - 1\n",
    "test_labels = torch.from_numpy(df_test['class_idx'].values) - 1\n",
    "\n",
    "# One-hot encode labels for training.\n",
    "train_target = torch.zeros((len(train_labels), 4))\n",
    "train_target = train_target.scatter_(1, train_labels.unsqueeze(1), 1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, it's time to define our network. The neural net consists of three fully connected layers (`fc1`, `fc2`, and `fc3`) with ReLU activation (`relu`) in between each layer. We flatten the input tensor using `view` before passing it through the fully connected layers. Finally, we apply the softmax activation function (`softmax`) to the output tensor to obtain the predicted probabilities for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicClassifier(nn.Module):\n",
    "    def __init__(self, input_width, input_length, output_size):\n",
    "        super(TopicClassifier, self).__init__()\n",
    "        self.input_width = input_width\n",
    "        self.input_length = input_length\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_width * input_length, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input tensor.\n",
    "        x = x.view(-1, self.input_width * self.input_length)\n",
    "        \n",
    "        # Pass through the fully connected layers with ReLU activation\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # Apply softmax activation to the output.\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to train our network, this may take a while, but the current loss will be printed after every epoch.\n",
    "If you want to run the code faster, you can also put this notebook on Google Colab and use its provided GPU to speed up computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.3808\n",
      "Epoch [2/5], Loss: 1.2921\n",
      "Epoch [3/5], Loss: 1.1658\n",
      "Epoch [4/5], Loss: 1.1610\n",
      "Epoch [5/5], Loss: 1.0698\n"
     ]
    }
   ],
   "source": [
    "# Define parameters.\n",
    "n_classes = len(train_labels.unique())\n",
    "input_size = input_train.shape[1:]\n",
    "num_epochs = 5\n",
    "lr = 0.001\n",
    "\n",
    "# Define model, loss function and optimizer.\n",
    "model = TopicClassifier(*input_size, output_size=n_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop.\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(zip(input_train, train_target)):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch [%d/%d], Loss: %.4f' % (epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"a7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment for Task 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your task (which is your assignment) is to do the following:**\n",
    "\n",
    "- Use the code below to evaluate the neural network. \n",
    "- Generate a confusion matrix with `sklearn.metrics.confusion_matrix` (it's already imported so you can call `confusion_matrix`). \n",
    "- Plot the confusion matrix using `seaborn.heatmap` (`seaborn` is usually imported as `sns`). Set the `annot` argument to `True` and the `xticklabels` and `yticklabels` to the `labels` list.\n",
    "- Also take the time to evaluate for the train set. Is there a notable difference in accuracy, recall and the F1 score between the train and test sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the neural net on the test set\n",
    "model.eval()\n",
    "\n",
    "# Sample from the model.\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(input_test)\n",
    "    # Use argmax to get the topic with highest probability.\n",
    "    test_pred = np.argmax(test_outputs.detach(), axis=1)\n",
    "\n",
    "# Set model back to training mode.\n",
    "model.train()\n",
    "\n",
    "labels = ['World', 'Sports', 'Business','Sci/Tech']\n",
    "\n",
    "###################################\n",
    "# Fill in your answer here\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"a8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Assignment / Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're not over all this text data yet, there's always more to do. You can still experiment with the number of epochs, learning rate, vector size, optimizer, neural network layers, regularization and so much more. \n",
    "Even during the preprocessing, we could have done some things differently, like making everything lowercase and removing punctuation. \n",
    "Be aware that every choice you make along the way trickles down into your pipeline and can have some effect on your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
